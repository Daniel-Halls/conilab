{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d98ad7c",
   "metadata": {},
   "source": [
    "# CA3 model\n",
    "\n",
    "This notebook is for building out the CA3 model as previous implementation doesn't work.\n",
    "\n",
    "Formula is:\n",
    "$$\n",
    "min_{\\vec{w}_{x_1}, \\vec{w}_{x_2}, \\vec{w}_a, \\vec{w}_b} \\left( -\\vec{w}_{x_1}^T S_{xa} \\vec{w}_a - \\vec{w}_{x_2}^T S_{xb} \\vec{w}_b + \\sum_{i \\in \\{x_1, x_2, a, b\\}} \\frac{1}{2} \\lambda_i \\left( \\vec{w}_i^T S_{ii} \\vec{w}_i - 1 \\right) + \\theta_{rr}(\\vec{w}_{x_1}, \\vec{w}_{x_2}) \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0861b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemmr.generative_model import GEMMR\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from gemmr.estimators import SVDCCA\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d9b9d",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf757017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = GEMMR('cca', wx=1, wy=1, r_between=0.3)\n",
    "behavioural_data_study1, imging_data_study1 = model_definition.generate_data(n=200)\n",
    "behavioural_data_study2, imging_data_study2 = model_definition.generate_data(n=190)\n",
    "study1 = (imging_data_study1, behavioural_data_study1) \n",
    "study2 = (imging_data_study2, behavioural_data_study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9231845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23474441])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= SVDCCA().fit(imging_data_study1, behavioural_data_study1)\n",
    "test.corrs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e8e3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum = test.transform(imging_data_study1, behavioural_data_study1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc9559",
   "metadata": {},
   "source": [
    "## Step 1: Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "523234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to demean data.\n",
    "\n",
    "    Parmeteres\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data to demean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        demeaned data\n",
    "    \"\"\"\n",
    "    return data - data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov(matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate \n",
    "    covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_1: np.ndarray\n",
    "        A matrix tht should \n",
    "        correspond to subject by \n",
    "        features\n",
    "    matrix_2: np.ndarray\n",
    "        A matrix that should \n",
    "        correspond to features by\n",
    "        feautres \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        array of cross covariance matrix\n",
    "    \"\"\"\n",
    "    return (matrix_1.T @ matrix_2) / matrix_1.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_able_to_process(study_pair: tuple, behav_data: np.ndarray, X_dat: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check that data\n",
    "    is in correct format to be processed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     study_pair: tuple, \n",
    "         tuple of behavioural data \n",
    "         and imging data\n",
    "     behav_data: np.ndarray\n",
    "         array of behav_data \n",
    "     X_dat: np.ndarray\n",
    "         array of X_dat\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool: boolean\n",
    "        bool of if failed or not\n",
    "    \"\"\"\n",
    "    if not isinstance(study_pair, (tuple, list)) or len(study_pair) != 2:\n",
    "        print(\"Given argument isn't a pair of datasets\")\n",
    "        return False\n",
    "    if not isinstance(behav_data, np.ndarray) or not isinstance(X_dat, np.ndarray):\n",
    "        print(\"Data provided isn't a numpy array\")\n",
    "        return False\n",
    "    if behav_data.shape[0] == 0 or X_dat.shape[0] == 0 or behav_data.shape[0] != X_dat.shape[0]:\n",
    "        print(f\"Mismatch between ({behav_data.shape[0]} and {X_dat.shape[0]})\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb76963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_matricies(*study_pairs) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates covariance matrices and auto covariance\n",
    "    matricies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_pairs: tuple\n",
    "        a tuple or list containing two numpy arrays:\n",
    "        (behavioural_data, imaging_data).\n",
    "        Assumes data is (subjects x features).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    covariance_results: dict\n",
    "        dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "    \"\"\"\n",
    "    covariance_results = {}\n",
    "    for idx, study_pair in enumerate(study_pairs):\n",
    "        X_dat, behav_data  = study_pair\n",
    "        if not data_able_to_process(study_pair, behav_data, X_dat):\n",
    "            continue\n",
    "        behav_data = mean_center(behav_data)\n",
    "        X_dat = mean_center(X_dat)\n",
    "        study_num = idx + 1\n",
    "        try:\n",
    "            covariance_results[f\"s_behav{study_num}_behav{study_num}\"] = cross_cov(behav_data, behav_data)\n",
    "            covariance_results[f\"s_img{study_num}_img{study_num}\"] = cross_cov(X_dat, X_dat)\n",
    "            covariance_results[f\"s_img{study_num}_behav{study_num}\"] = cross_cov(X_dat, behav_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "            return None\n",
    "    return covariance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd37f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_mat = calculate_covariance_matricies(study1, study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36418b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x1a = cross_cov(behavioural_data_study1, imging_data_study1)\n",
    "s_x2b = cross_cov(behavioural_data_study2, imging_data_study2)\n",
    "s_x1x1 = cross_cov(behavioural_data_study1, behavioural_data_study1)\n",
    "s_x2x2 = cross_cov(behavioural_data_study2, behavioural_data_study2)\n",
    "s_aa = cross_cov(imging_data_study1, imging_data_study1)\n",
    "s_bb = cross_cov(imging_data_study2, imging_data_study2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040469c",
   "metadata": {},
   "source": [
    "## Step 2. Intialization of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9220d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_intialization(*weights) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Define a set of random starting \n",
    "    weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: tuple(int)\n",
    "        tuple of set amount\n",
    "        of int values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarrray\n",
    "        array of numpy values\n",
    "    \"\"\"\n",
    "    return np.random.randn(sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d329c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = {\n",
    "  '1': behavioural_data_study1.shape[1], \n",
    "   '2': behavioural_data_study2.shape[1],\n",
    "    '3': imging_data_study1.shape[1],\n",
    "    '4': imging_data_study2.shape[1]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6c66dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = weight_intialization(\n",
    "    behavioural_data_study1.shape[1], \n",
    "    behavioural_data_study2.shape[1],\n",
    "    imging_data_study1.shape[1],\n",
    "    imging_data_study2.shape[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b44fd",
   "metadata": {},
   "source": [
    "## Step 3. Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2db5333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx1_shape = s_x1x1.shape[0]\n",
    "dx2_shape = s_x2x2.shape[0]\n",
    "da_shape = s_aa.shape[0]\n",
    "db_shape = s_bb.shape[0]\n",
    "dx_shape = dx1_shape + dx2_shape\n",
    "dac_shape =  dx_shape + da_shape\n",
    "\n",
    "def get_dimensions(s_x1x1, s_x2x2, s_aa):\n",
    "    dx1_shape = s_x1x1.shape[0]\n",
    "    dx2_shape = s_x2x2.shape[0]\n",
    "    da_shape = s_aa.shape[0]\n",
    "    dx_shape =  dx1_shape + dx2_shape\n",
    "    return {\n",
    "        'dx1_shape': dx1_shape,\n",
    "        'dx_shape' : dx_shape,\n",
    "        'dac_shape':  dx_shape + da_shape\n",
    "    }\n",
    "\n",
    "def get_weights(weight_array, dx1_shape, dx_shape, dac_shape):\n",
    "    return {\n",
    "        \"wx1\": weight_array[:dx1_shape],\n",
    "        \"wx2\": weight_array[dx1_shape:dx_shape],\n",
    "        \"wa\": weight_array[dx_shape:dac_shape],\n",
    "        \"wb\": weight_array[dac_shape:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43838fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov_term(weight_beh, cov_mat, weight_img):\n",
    "    return -weight_img.T @ (cov_mat @ weight_beh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a0332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_term(weight, cov_mat):\n",
    "    return 0.5 * 1.0 * (weight.T @ (cov_mat @ weight) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeeb8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity_penality(theta_r, img_weight1, img_weight2):\n",
    "    return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79bec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(weights, s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r):\n",
    "    dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "    weights = get_weights(\n",
    "            weights, \n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])\n",
    "    term1 = cross_cov_term(weights['wx1'],s_x1a, weights['wa'])\n",
    "    term2 = cross_cov_term(weights['wx2'],s_x2b, weights['wb'])\n",
    "    reg_x1 = regularization_term(weights['wx1'], s_x1x1)\n",
    "    reg_x2 = regularization_term(weights['wx2'], s_x2x2)\n",
    "    reg_a = regularization_term(weights['wa'], s_aa)\n",
    "    reg_b = regularization_term(weights['wb'], s_bb)\n",
    "    theta_r = dissimilarity_penality(theta_r, weights['wa'], weights['wb'])\n",
    "    return term1 + term2 + reg_x1 + reg_x2 + reg_a + reg_b + theta_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9f1c9",
   "metadata": {},
   "source": [
    "## Step 4: Minimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3de1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "optimal_theta_r = None\n",
    "optimium_model = None\n",
    "\n",
    "for theta_r in np.logspace(-3, 2, 10):\n",
    "    weights_0 = weight_intialization(\n",
    "        behavioural_data_study1.shape[1], \n",
    "        behavioural_data_study2.shape[1],\n",
    "        imging_data_study1.shape[1],\n",
    "        imging_data_study2.shape[1])  # re-init each time\n",
    "    res = minimize(\n",
    "        objective_function,\n",
    "        weights_0,\n",
    "        args=(s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r),\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    if res.status !=0:\n",
    "        print(res.status)\n",
    "        continue\n",
    "\n",
    "\n",
    "    if res.fun < best_loss:\n",
    "        best_loss = res.fun\n",
    "        best_theta_r = theta_r\n",
    "        optimium_model = res\n",
    "\n",
    "print(f\"Best Î¸_r: {best_theta_r}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "weights = get_weights(optimium_model.x,\n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d427de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get projections (scores)\n",
    "scores_x1 = behavioural_data_study1 @ weights['wx1']\n",
    "scores_x2 = behavioural_data_study2 @ weights['wx2']\n",
    "scores_a  = imging_data_study1 @ weights['wa']\n",
    "scores_b  = imging_data_study2 @ weights['wb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11538425261898907)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.07889484464506155)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.00014508777456870848)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.corrcoef(scores_x1, scores_a)[0, 1])\n",
    "display(np.corrcoef(scores_x2, scores_b)[0, 1])\n",
    "display(np.linalg.norm(weights['wa'] - weights['wb']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfca62",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridSearchCA3:\n",
    "    def __init__(self, l2_values, theta=0, tol=1e-6, maxiter=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Custom grid search to find the best l2 value for the CA3 model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l2_values : list of float\n",
    "            The l2 regularization parameters to search over.\n",
    "        theta : float\n",
    "            The dissimilarity regularization parameter (shared across all models).\n",
    "        tol : float\n",
    "            Tolerance for optimization.\n",
    "        maxiter : int\n",
    "            Maximum number of optimization iterations.\n",
    "        verbose : bool\n",
    "            If True, print progress during search.\n",
    "        \"\"\"\n",
    "        self.l2_values = l2_values\n",
    "        self.theta = theta\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "        self.verbose = verbose\n",
    "        self.best_model_ = None\n",
    "        self.best_score_ = -np.inf\n",
    "        self.best_l2_ = None\n",
    "        self.all_results_ = []\n",
    "\n",
    "    def fit(self, *data_sets):\n",
    "        \"\"\"\n",
    "        Fit CA3 models with each l2 value and track the one with best score.\n",
    "        \"\"\"\n",
    "        for l2 in self.l2_values:\n",
    "            model = C3A(l2=l2, theta=self.theta, tol=self.tol, maxiter=self.maxiter)\n",
    "            model.fit(*data_sets)\n",
    "            score = model._score(*data_sets)\n",
    "\n",
    "            self.all_results_.append((l2, score))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"l2: {l2:.4f}, score: {score:.4f}\")\n",
    "\n",
    "            if score > self.best_score_:\n",
    "                self.best_score_ = score\n",
    "                self.best_model_ = model\n",
    "                self.best_l2_ = l2\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Best l2: {self.best_l2_:.4f}, Best score: {self.best_score_:.4f}\")\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model_\n",
    "\n",
    "    def get_best_l2(self):\n",
    "        return self.best_l2_\n",
    "\n",
    "    def get_all_results(self):\n",
    "        return self.all_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c072048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class C3A:\n",
    "    \"\"\"\n",
    "    C3A class.\n",
    "    A class to do C3A\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    c3a = C3A(l2=0.5, theta=1)\n",
    "    c3a.fit(study1, study2)\n",
    "    transformed = c3a.transform(study1, study2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2: float = 1,\n",
    "        theta: float = 0,\n",
    "        tol=1e-6,\n",
    "        maxiter=500,\n",
    "        normalise_weights=True,\n",
    "    ):\n",
    "        self.l2_ = l2\n",
    "        self.theta_ = theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        self.tol_ = tol\n",
    "        self.maxiter_ = maxiter\n",
    "        self.normalise_weights = normalise_weights\n",
    "        self.canonical_correlations_ = None\n",
    "        self.projections_ = None\n",
    "\n",
    "    def fit(self, *data_sets: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Method to fit the CA3 model to a given\n",
    "        set of datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._calculate_covariance_matricies(*data_sets)\n",
    "        self._get_dimensions(*data_sets)\n",
    "        self._weight_intialization()\n",
    "        self._optimise()\n",
    "\n",
    "    def transform(self, *data_sets: tuple) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to transform data sets into canonical\n",
    "        projects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.weights_ is not None\n",
    "        ), \"Model must be fitted before transform can be called.\"\n",
    "        assert len(data_sets) == len(\n",
    "            self.dims_\n",
    "        ), \"Model fitted with different number of datasets.\"\n",
    "\n",
    "        self.projections_ = [\n",
    "            np.stack(\n",
    "                [\n",
    "                    self._normalise(\n",
    "                        self._normalise(X_data) @ wx)\n",
    "                    if self.normalise_weights\n",
    "                    else X_data @ wx,\n",
    "                    self._normalise(self._normalise(Y_data) @ wb)\n",
    "                    if self.normalise_weights\n",
    "                    else Y_data @ wb,\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "            for (X_data, Y_data), (wx, wb) in zip(data_sets, self.weights_)\n",
    "        ]\n",
    "\n",
    "        self.canonical_correlations_ = [\n",
    "            np.corrcoef(data_sets[0], data_sets[1])[0, 1] for data_sets in self.projections_\n",
    "        ]\n",
    "        return self.projections_\n",
    "\n",
    "    def fit_transform(self, *data_sets) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to fit a CA3 model and then transform\n",
    "        the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples.\n",
    "        \"\"\"\n",
    "        self.fit(*data_sets)\n",
    "        return self.transform(*data_sets)\n",
    "\n",
    "    def calculate_canonical_correlations(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Method to obtain the canonical correlations.\n",
    "        Model must have been fitted and transfomed\n",
    "        before.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        canonical_correlations: list[float]\n",
    "            list of canonical correlations\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.canonical_correlations_ is not None\n",
    "        ), \"Model must be fitted and transfomed before correlations can be returned\"\n",
    "        return self.canonical_correlations_\n",
    "        \n",
    "    def compute_loadings(self, *data_sets: tuple) -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Computes canonical loadings for each study.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (img_data, beh_data) pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loadings: list of tuples\n",
    "            Each tuple contains (img_loadings, beh_loadings), i.e., correlations between\n",
    "            original features and their respective canonical variates.\n",
    "        \"\"\"\n",
    "        assert self.projections_ is not None, \"Model must be fitted and transfomed before computing loadings.\"\n",
    "        return [\n",
    "        (\n",
    "            np.corrcoef(self._normalise(X_data).T, x_proj, rowvar=True)[:-1, -1],\n",
    "            np.corrcoef(self._normalise(Y_data).T, y_proj, rowvar=True)[:-1, -1]\n",
    "        )\n",
    "        for (X_data, Y_data), (x_proj, y_proj) in zip(data_sets, self.projections_)\n",
    "    ]\n",
    "    \n",
    "    def _weight_intialization(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to define a set of random starting\n",
    "        weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\"\n",
    "        init_weights = []\n",
    "\n",
    "        for idx, _ in enumerate(self.dims_):\n",
    "            s_xb = self.covariances_[f\"s_X{idx+1}_Y{idx+1}\"]\n",
    "            # Perform SVD on the cross-covariance matrix\n",
    "            try:\n",
    "                U, _, Vt = np.linalg.svd(s_xb, full_matrices=False)\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                raise RuntimeError(f\"SVD failed for dataset {idx+1}: {e}\")\n",
    "\n",
    "            wx = U[:, 0]\n",
    "            wb = Vt.T[:, 0]\n",
    "            s_xx = self.covariances_[f\"s_X{idx+1}_X{idx+1}\"]\n",
    "            s_bb = self.covariances_[f\"s_Y{idx+1}_Y{idx+1}\"]\n",
    "\n",
    "            wx = wx / np.sqrt(wx.T @ s_xx @ wx)\n",
    "            wb = wb / np.sqrt(wb.T @ s_bb @ wb)\n",
    "\n",
    "            init_weights.extend(wx)\n",
    "            init_weights.extend(wb)\n",
    "\n",
    "        self.intial_weights_ = np.array(init_weights)\n",
    "\n",
    "    def _calculate_covariance_matricies(self, *data_sets) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance and auto covariance\n",
    "        matricies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "        \"\"\"\n",
    "        for idx, study_pair in enumerate(data_sets):\n",
    "            X_data, Y_data = study_pair\n",
    "            self._data_able_to_process(study_pair)\n",
    "            X_data = self._normalise(X_data)\n",
    "            Y_data = self._normalise(Y_data)\n",
    "            study_num = idx + 1\n",
    "            try:\n",
    "                self.covariances_[f\"s_Y{study_num}_Y{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(Y_data, Y_data)\n",
    "                )\n",
    "                self.covariances_[f\"s_X{study_num}_X{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(X_data, X_data)\n",
    "                )\n",
    "                self.covariances_[f\"s_X{study_num}_Y{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(X_data, Y_data)\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, study_pair: tuple) -> bool:\n",
    "        \"\"\"\n",
    "        Method to check that data\n",
    "        is in correct format to be processed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple,\n",
    "             tuple of behavioural data\n",
    "             and imging data\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            isinstance(study_pair, (tuple, list)) and len(study_pair) == 2\n",
    "        ), \"Given argument isn't a pair of datasets\"\n",
    "        assert isinstance(study_pair[0], np.ndarray) or not isinstance(\n",
    "            study_pair[1], np.ndarray\n",
    "        ), \"Data provided ins't numpy array\"\n",
    "        assert (study_pair[0].shape[0] != 0) and (\n",
    "            study_pair[1].shape[0] != 0\n",
    "        ), \"Study pairs contains not data\"\n",
    "        assert (\n",
    "            study_pair[0].shape[0] == study_pair[1].shape[0]\n",
    "        ), f\"Mismatch between ({study_pair[0].shape[0]} and {study_pair[1].shape[0]})\"\n",
    "\n",
    "    def _optimise(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to minimise the\n",
    "        objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        model = minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            options={\"gtol\": self.tol_, \"maxiter\": self.maxiter_},\n",
    "            args=(self.covariances_, self.theta_, self.l2_),\n",
    "        )\n",
    "        self.best_loss = model.fun\n",
    "        self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "    def _get_dimensions(self, *data_sets) -> None:\n",
    "        \"\"\"\n",
    "        Method to get the dimensions\n",
    "        of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *data_sets: tuple\n",
    "            tuple of datasets\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dims_ = [(Y.shape[1], X.shape[1]) for X, Y in data_sets]\n",
    "\n",
    "    def _split_weights(self, weights: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector weights into individual vectors\n",
    "        for each x and b dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            flatten numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        split_weights: list[np.ndarray]\n",
    "            list of weights split\n",
    "            wx and wb\n",
    "\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        split_weights = []\n",
    "        for X_dim, Y_dim in self.dims_:\n",
    "            wx = weights[offset : offset + X_dim]\n",
    "            offset += X_dim\n",
    "            wb = weights[offset : offset + Y_dim]\n",
    "            offset += Y_dim\n",
    "            split_weights.append((wx, wb))\n",
    "        return split_weights\n",
    "\n",
    "    def _objective_function(\n",
    "        self, weights: np.ndarray, covariances: dict, theta: float, l2: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Objective function of the CA3 class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            weights\n",
    "        covariances: dict\n",
    "            dict of cross/auto covariance\n",
    "            matricies\n",
    "        theta: float\n",
    "            theta penality\n",
    "        l2: float\n",
    "            regularization penailty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        total_loss: float\n",
    "           total loss of the objective function\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        weights_ = self._split_weights(weights)\n",
    "        for idx, (wx, wb) in enumerate(weights_):\n",
    "            s_xb = covariances[f\"s_X{idx+1}_Y{idx+1}\"]\n",
    "            s_xx = covariances[f\"s_X{idx+1}_X{idx+1}\"]\n",
    "            s_bb = covariances[f\"s_Y{idx+1}_Y{idx+1}\"]\n",
    "            total_loss += self._cross_cov_term(wb, s_xb, wx)\n",
    "            total_loss += self._regularization_term(wx, s_xx, l2)\n",
    "            total_loss += self._regularization_term(wb, s_bb, l2)\n",
    "\n",
    "        # Similarity penalty across imaging weights\n",
    "        if theta > 0 and len(weights_) > 1:\n",
    "            total_loss += sum(\n",
    "                self._dissimilarity_penality(theta, w1[0], w2[0])\n",
    "                for w1, w2 in combinations(weights_, 2)\n",
    "            )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def _create_covariance_matrix(\n",
    "        self, matrix_1: np.ndarray, matrix_2: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cross-auto\n",
    "        covariance matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should\n",
    "            correspond to subject by\n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should\n",
    "            correspond to features by\n",
    "            feautres\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0]\n",
    "\n",
    "    def _normalise(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to normalise data.\n",
    "\n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        dmean = data - data.mean(axis=0)\n",
    "        std = data.std(axis=0, ddof=1)\n",
    "        std = np.where(std == 0.0, 1.0, std)\n",
    "        return dmean / std\n",
    "\n",
    "    def _cross_cov_term(\n",
    "        self, weight_Y: np.ndarray, cov_mat: np.ndarray, weight_X: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to calculate the cross covarance term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_Y: np.ndarray\n",
    "            set of weights for wb\n",
    "        cov_mat: np.ndarray\n",
    "             covariance matrix for\n",
    "             wx wb\n",
    "        weight_X: np.ndarray\n",
    "            set of weights for wx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: np.array\n",
    "            cross covariance term\n",
    "        \"\"\"\n",
    "        return -weight_X.T @ (cov_mat @ weight_Y)\n",
    "\n",
    "    def _regularization_term(\n",
    "        self, weight: np.ndarray, cov_mat: np.ndarray, lambda_i: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the regularization term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight: np.ndarray\n",
    "            set of weights\n",
    "        cov_mat: np.ndarray\n",
    "            auto covariance matrix\n",
    "        lambda_i: float\n",
    "            regularization parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            regularization term of the objective function\n",
    "        \"\"\"\n",
    "        return 0.5 * lambda_i * (weight.T @ (cov_mat @ weight) - 1)\n",
    "\n",
    "    def _dissimilarity_penality(\n",
    "        self, theta_r: float, X_weight1: np.ndarray, X_weight2: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to return dissimilarity penality\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        theta_r: float\n",
    "           theta penality.\n",
    "        img_weight1: np.ndarray\n",
    "            weights of imaging data\n",
    "        img_weight2: np.ndarray\n",
    "            weights of second imaging\n",
    "            data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            dissimilarity penality\n",
    "        \"\"\"\n",
    "        return theta_r * 0.5 * np.sum((X_weight1 - X_weight2) ** 2)\n",
    "\n",
    "    def _score(self, *data_sets: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Method used to evaluate model performance.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            mean of correlation\n",
    "            values across datasets\n",
    "\n",
    "        \"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before scoring.\")\n",
    "\n",
    "        self.transform(*data_sets)\n",
    "        correlations = self.calculate_canonical_correlations()\n",
    "        return np.mean(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5254108",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca3 = C3A(l2=0.0)\n",
    "ca3.fit(study1, study2)\n",
    "transfomed = ca3.fit_transform(study1, study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94dceee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 190)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "546ebe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7591518749496465)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomed[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b23471a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1.]), array([1.]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.compute_loadings(study1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "982f07fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74427671])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002ccd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.20288757])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.transform(study1[0], study1[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fb32f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_score = transfomed[0][0]\n",
    "xk = ca3._normalise(study1[0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05372a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loadings = np.dot(x_score, xk) / np.dot(x_score, x_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc859485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c7babe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([389.43112117])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.weights_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e86fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study1[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e23cb944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([389.43112117])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.weights_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c88d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_weights = np.zeros((study1[0].shape[1], 1))\n",
    "x_weights[:, 0] = ca3.weights_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99c8008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rotation = np.dot(x_weights, \n",
    "                    pinv(np.dot(x_loadings.T, x_weights),check_finite=False)\n",
    "                    , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86f2a22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b84e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (study1[0] - study1[0].mean()) #- study1[0].std()\n",
    "x /= study1[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1098ac92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0317379170565095)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study1[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23cdb820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08232184])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, sk.x_rotations_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a80bb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.20288757e+00],\n",
       "        [ 9.22649973e-01],\n",
       "        [-1.10329874e-01],\n",
       "        [ 1.28439865e-01],\n",
       "        [-1.07443166e+00],\n",
       "        [-3.26648917e-01],\n",
       "        [-1.79658908e+00],\n",
       "        [ 3.36288050e-01],\n",
       "        [ 1.59393479e-01],\n",
       "        [ 8.38276639e-01],\n",
       "        [-4.71472021e-01],\n",
       "        [ 1.51484404e-03],\n",
       "        [-3.88391862e-01],\n",
       "        [ 1.75429865e-01],\n",
       "        [ 8.88828295e-01],\n",
       "        [ 2.00723843e+00],\n",
       "        [ 9.78067199e-01],\n",
       "        [ 6.81175830e-01],\n",
       "        [-7.17943876e-01],\n",
       "        [-6.69205971e-01],\n",
       "        [ 8.66421829e-01],\n",
       "        [ 1.42287735e+00],\n",
       "        [ 1.29917942e-02],\n",
       "        [ 8.57429069e-01],\n",
       "        [-7.29835571e-01],\n",
       "        [-1.64573071e+00],\n",
       "        [ 8.73549663e-01],\n",
       "        [-1.36669747e+00],\n",
       "        [-2.50207515e-01],\n",
       "        [-2.53162743e+00],\n",
       "        [ 7.84819547e-01],\n",
       "        [-1.54139025e-02],\n",
       "        [-1.40266295e+00],\n",
       "        [ 1.09396836e+00],\n",
       "        [-7.16348144e-01],\n",
       "        [ 1.78474276e+00],\n",
       "        [-1.04422751e+00],\n",
       "        [ 9.94477859e-01],\n",
       "        [ 1.21167439e+00],\n",
       "        [ 3.17620775e-01],\n",
       "        [-9.09543851e-01],\n",
       "        [ 1.21821394e+00],\n",
       "        [-1.02796429e+00],\n",
       "        [-8.89349795e-02],\n",
       "        [-3.49036635e-01],\n",
       "        [-1.89597438e+00],\n",
       "        [ 9.18334439e-01],\n",
       "        [-8.21339423e-01],\n",
       "        [-1.70080173e+00],\n",
       "        [ 2.86910463e-01],\n",
       "        [-5.27224929e-02],\n",
       "        [ 5.15214623e-01],\n",
       "        [-1.46222691e-01],\n",
       "        [ 1.49592693e-01],\n",
       "        [ 7.65713590e-01],\n",
       "        [ 8.39739190e-01],\n",
       "        [ 1.15513931e+00],\n",
       "        [-1.19620624e+00],\n",
       "        [ 7.59279995e-01],\n",
       "        [-3.01565744e-02],\n",
       "        [ 7.17494685e-01],\n",
       "        [-1.95504042e+00],\n",
       "        [ 1.26390118e+00],\n",
       "        [ 1.50428877e+00],\n",
       "        [-7.56984201e-03],\n",
       "        [ 7.15618285e-03],\n",
       "        [ 1.95355100e-01],\n",
       "        [ 2.84266019e-01],\n",
       "        [ 2.49264213e-01],\n",
       "        [-1.83407286e+00],\n",
       "        [-5.42870253e-01],\n",
       "        [-7.65343486e-01],\n",
       "        [ 2.19583268e+00],\n",
       "        [-1.24637311e+00],\n",
       "        [-2.04253021e+00],\n",
       "        [-2.86578040e-01],\n",
       "        [-6.84917820e-01],\n",
       "        [-3.92059350e-01],\n",
       "        [ 7.20320190e-01],\n",
       "        [ 6.66991603e-01],\n",
       "        [ 1.56502584e-02],\n",
       "        [-1.38537718e-01],\n",
       "        [ 2.64618354e-01],\n",
       "        [-4.83130786e-01],\n",
       "        [-1.98966597e+00],\n",
       "        [-2.13587521e+00],\n",
       "        [ 9.86275931e-01],\n",
       "        [ 1.20520624e-01],\n",
       "        [-4.99765792e-01],\n",
       "        [ 9.13040052e-01],\n",
       "        [-1.08041146e+00],\n",
       "        [ 6.35698235e-01],\n",
       "        [-1.01048129e+00],\n",
       "        [ 6.90004785e-01],\n",
       "        [-6.40576331e-01],\n",
       "        [ 3.46649303e-01],\n",
       "        [ 3.07429484e-01],\n",
       "        [-5.75420922e-01],\n",
       "        [-4.10167407e-02],\n",
       "        [-4.31948746e-01],\n",
       "        [ 2.22658276e+00],\n",
       "        [ 8.65901757e-01],\n",
       "        [ 4.28221255e-01],\n",
       "        [-6.16621764e-01],\n",
       "        [ 5.52339485e-01],\n",
       "        [ 6.36028428e-01],\n",
       "        [-2.10281131e-01],\n",
       "        [-3.85299372e-01],\n",
       "        [-1.28774507e+00],\n",
       "        [ 2.87275904e+00],\n",
       "        [ 5.96156014e-01],\n",
       "        [ 7.46023488e-01],\n",
       "        [ 4.80272262e-01],\n",
       "        [ 2.55019497e+00],\n",
       "        [-7.20903526e-01],\n",
       "        [-5.85506879e-01],\n",
       "        [ 8.43465368e-01],\n",
       "        [-2.42315229e-01],\n",
       "        [-4.64556035e-01],\n",
       "        [ 6.84775174e-01],\n",
       "        [ 6.00536874e-01],\n",
       "        [-3.19964395e-02],\n",
       "        [ 2.80375099e-01],\n",
       "        [-3.50031841e-01],\n",
       "        [-5.20115654e-01],\n",
       "        [ 6.84352992e-01],\n",
       "        [ 1.70418266e+00],\n",
       "        [-3.30337712e-01],\n",
       "        [-6.31001339e-02],\n",
       "        [-2.19632376e-01],\n",
       "        [-1.73889833e-01],\n",
       "        [-1.94960600e+00],\n",
       "        [ 3.76705455e-01],\n",
       "        [-2.31871620e+00],\n",
       "        [-8.68610259e-01],\n",
       "        [-7.86215373e-02],\n",
       "        [ 8.06336895e-01],\n",
       "        [-8.16182152e-01],\n",
       "        [-1.18743466e+00],\n",
       "        [ 1.34880185e+00],\n",
       "        [-1.18274104e-01],\n",
       "        [-8.99215340e-01],\n",
       "        [ 2.51885288e-01],\n",
       "        [-1.30886818e+00],\n",
       "        [-5.71963375e-01],\n",
       "        [ 4.88238768e-01],\n",
       "        [ 9.45910238e-01],\n",
       "        [ 1.54187842e-01],\n",
       "        [-1.08427408e+00],\n",
       "        [-4.06004935e-01],\n",
       "        [-2.26212051e+00],\n",
       "        [ 1.13085765e+00],\n",
       "        [-4.76817563e-02],\n",
       "        [ 1.70415976e+00],\n",
       "        [ 7.62518777e-01],\n",
       "        [ 9.04072866e-01],\n",
       "        [ 1.53933872e+00],\n",
       "        [-1.21310219e+00],\n",
       "        [-1.58242878e+00],\n",
       "        [ 2.59920508e-01],\n",
       "        [-5.52375403e-01],\n",
       "        [ 4.29549196e-01],\n",
       "        [-5.70126410e-01],\n",
       "        [ 5.77200132e-01],\n",
       "        [-4.13970417e-01],\n",
       "        [-1.97192511e+00],\n",
       "        [ 3.38374800e-01],\n",
       "        [ 1.86334695e+00],\n",
       "        [ 4.82358290e-01],\n",
       "        [-3.65244061e-01],\n",
       "        [-1.84327474e+00],\n",
       "        [ 9.11967402e-01],\n",
       "        [ 4.02527683e-01],\n",
       "        [ 1.37539831e-01],\n",
       "        [-7.52244852e-01],\n",
       "        [ 2.25488832e-01],\n",
       "        [-1.40931199e-01],\n",
       "        [ 3.63880017e-02],\n",
       "        [ 1.05386303e+00],\n",
       "        [ 4.77408947e-01],\n",
       "        [ 1.55234295e+00],\n",
       "        [-6.93340728e-01],\n",
       "        [-6.13745983e-01],\n",
       "        [ 7.07392075e-01],\n",
       "        [ 6.14681244e-01],\n",
       "        [-6.83369444e-01],\n",
       "        [ 3.70918796e-01],\n",
       "        [ 1.32095434e+00],\n",
       "        [ 1.63460658e+00],\n",
       "        [-3.66478695e-01],\n",
       "        [-6.46064175e-01],\n",
       "        [-9.70203236e-01],\n",
       "        [ 5.08056932e-01],\n",
       "        [-1.94178859e+00],\n",
       "        [-9.97970010e-01],\n",
       "        [-2.02020911e-01],\n",
       "        [ 3.42877052e-01],\n",
       "        [ 1.56168102e-01],\n",
       "        [ 3.87890534e-01],\n",
       "        [-2.89298125e-01]]),\n",
       " array([[ 4.23081429e-01],\n",
       "        [ 1.40705821e-01],\n",
       "        [ 2.95440875e+00],\n",
       "        [-6.91899841e-01],\n",
       "        [-3.60625845e-01],\n",
       "        [ 3.60249147e-01],\n",
       "        [-1.72130459e+00],\n",
       "        [ 1.03414323e+00],\n",
       "        [-6.02032497e-01],\n",
       "        [ 1.30197128e-01],\n",
       "        [-4.50697273e-01],\n",
       "        [ 6.80604626e-02],\n",
       "        [-2.57187573e+00],\n",
       "        [ 4.96367632e-01],\n",
       "        [ 3.61419674e-01],\n",
       "        [-3.85436595e-01],\n",
       "        [ 9.51016487e-01],\n",
       "        [-1.15230820e+00],\n",
       "        [ 1.26291834e+00],\n",
       "        [-1.80361471e+00],\n",
       "        [ 6.02067191e-01],\n",
       "        [ 2.55833151e-01],\n",
       "        [-7.30223179e-01],\n",
       "        [ 8.32125296e-01],\n",
       "        [ 4.22308183e-01],\n",
       "        [ 2.24101567e-01],\n",
       "        [-3.18200974e-01],\n",
       "        [ 1.01380317e+00],\n",
       "        [ 1.25654537e+00],\n",
       "        [ 1.02798432e+00],\n",
       "        [-8.53812835e-02],\n",
       "        [-7.74448405e-01],\n",
       "        [ 1.40190083e+00],\n",
       "        [ 8.54217306e-01],\n",
       "        [-4.72103919e-01],\n",
       "        [-9.73107334e-02],\n",
       "        [-6.02235916e-01],\n",
       "        [-4.66814458e-01],\n",
       "        [ 9.69620315e-01],\n",
       "        [-1.15746313e+00],\n",
       "        [-1.47542597e+00],\n",
       "        [-6.11764518e-01],\n",
       "        [ 1.55862069e+00],\n",
       "        [ 9.86471500e-02],\n",
       "        [ 3.12103777e-01],\n",
       "        [-5.21536641e-01],\n",
       "        [ 4.12372596e-01],\n",
       "        [-4.75312169e-01],\n",
       "        [ 2.26968956e-01],\n",
       "        [-8.93868472e-01],\n",
       "        [ 1.43784696e+00],\n",
       "        [-4.35601045e-01],\n",
       "        [-3.93228589e-01],\n",
       "        [ 1.93780863e+00],\n",
       "        [-9.14817355e-01],\n",
       "        [-9.44364263e-01],\n",
       "        [-1.18625416e+00],\n",
       "        [ 5.74350165e-01],\n",
       "        [ 4.78515464e-01],\n",
       "        [ 9.62841248e-01],\n",
       "        [-1.01182209e-01],\n",
       "        [-8.74969675e-01],\n",
       "        [ 9.22237195e-01],\n",
       "        [ 1.40129913e+00],\n",
       "        [-8.95715539e-02],\n",
       "        [ 1.02512478e+00],\n",
       "        [ 2.33043929e-01],\n",
       "        [-1.44208117e-01],\n",
       "        [ 1.96039518e+00],\n",
       "        [-1.16674584e+00],\n",
       "        [-1.67767665e+00],\n",
       "        [-1.22181035e+00],\n",
       "        [ 2.41307678e+00],\n",
       "        [ 4.79460899e-01],\n",
       "        [-1.51593448e-01],\n",
       "        [ 7.69033070e-01],\n",
       "        [-2.25496141e+00],\n",
       "        [-3.27624089e-01],\n",
       "        [ 6.84411847e-03],\n",
       "        [ 1.39353177e+00],\n",
       "        [-2.13351375e-01],\n",
       "        [ 1.10648251e+00],\n",
       "        [ 4.25406792e-01],\n",
       "        [-1.65590046e+00],\n",
       "        [-1.37204219e+00],\n",
       "        [-3.00813807e+00],\n",
       "        [ 2.05253162e+00],\n",
       "        [-4.06561954e-01],\n",
       "        [-2.66866442e+00],\n",
       "        [-7.68422576e-01],\n",
       "        [ 4.23361301e-01],\n",
       "        [-1.33165871e-01],\n",
       "        [ 1.42465985e-01],\n",
       "        [-3.87816585e-01],\n",
       "        [-1.82197962e+00],\n",
       "        [-3.15891071e-01],\n",
       "        [ 4.36946726e-01],\n",
       "        [-1.62339540e+00],\n",
       "        [-2.12229671e-01],\n",
       "        [-2.29595630e-02],\n",
       "        [-1.97489217e-01],\n",
       "        [ 9.49613721e-01],\n",
       "        [-6.77593496e-01],\n",
       "        [ 1.31448815e+00],\n",
       "        [ 3.44752160e-01],\n",
       "        [ 1.98022587e+00],\n",
       "        [ 2.18401618e+00],\n",
       "        [-5.00040132e-01],\n",
       "        [-3.14671343e-01],\n",
       "        [-4.11489170e-03],\n",
       "        [-3.52268904e-01],\n",
       "        [-1.18034279e-03],\n",
       "        [ 1.27430123e+00],\n",
       "        [ 1.40102226e+00],\n",
       "        [ 2.25876538e-02],\n",
       "        [-1.08013282e+00],\n",
       "        [ 5.38034655e-01],\n",
       "        [ 1.75438592e-01],\n",
       "        [-1.03939897e+00],\n",
       "        [-9.37344180e-01],\n",
       "        [ 7.03914939e-01],\n",
       "        [-5.61592931e-01],\n",
       "        [ 6.47603428e-01],\n",
       "        [-1.01034998e+00],\n",
       "        [-3.24187817e-01],\n",
       "        [-9.82540557e-01],\n",
       "        [ 1.23194647e-01],\n",
       "        [ 6.17404276e-01],\n",
       "        [-4.95788498e-01],\n",
       "        [-8.88558529e-02],\n",
       "        [-9.01965594e-01],\n",
       "        [-2.34529906e+00],\n",
       "        [ 5.95645984e-01],\n",
       "        [ 4.23436555e-01],\n",
       "        [-9.58143498e-01],\n",
       "        [-6.61239701e-01],\n",
       "        [ 1.44662739e+00],\n",
       "        [-2.99044093e-01],\n",
       "        [-1.23250732e+00],\n",
       "        [-9.98300232e-01],\n",
       "        [-4.11153436e-01],\n",
       "        [ 6.36551845e-01],\n",
       "        [ 1.34318029e+00],\n",
       "        [-6.07033961e-01],\n",
       "        [ 1.13321934e+00],\n",
       "        [-5.45204132e-01],\n",
       "        [-5.26107995e-01],\n",
       "        [-1.44734005e+00],\n",
       "        [-8.83669486e-02],\n",
       "        [-2.63222767e-01],\n",
       "        [-3.86835824e-01],\n",
       "        [ 1.99210777e+00],\n",
       "        [ 1.44499819e+00],\n",
       "        [ 1.30090321e+00],\n",
       "        [ 8.94568182e-01],\n",
       "        [ 4.35121912e-01],\n",
       "        [-5.07758462e-01],\n",
       "        [ 1.25009001e+00],\n",
       "        [-1.05339102e+00],\n",
       "        [-4.79636655e-01],\n",
       "        [ 5.93525753e-01],\n",
       "        [-2.52365784e-01],\n",
       "        [-3.74035111e-01],\n",
       "        [ 4.42083778e-01],\n",
       "        [-1.96159987e+00],\n",
       "        [-4.18415442e-01],\n",
       "        [-8.69107434e-02],\n",
       "        [-4.76034286e-01],\n",
       "        [-4.32013904e-02],\n",
       "        [ 1.41306591e+00],\n",
       "        [-1.35954090e+00],\n",
       "        [ 7.39202232e-01],\n",
       "        [ 2.31653281e+00],\n",
       "        [ 7.73713635e-02],\n",
       "        [-1.10293979e+00],\n",
       "        [ 5.91458657e-01],\n",
       "        [ 2.71469271e-01],\n",
       "        [ 2.82389746e-01],\n",
       "        [ 9.50181728e-02],\n",
       "        [ 9.97175815e-01],\n",
       "        [ 3.07103621e-01],\n",
       "        [ 1.73388674e-01],\n",
       "        [ 5.79044079e-01],\n",
       "        [ 2.42789668e-01],\n",
       "        [-5.67320300e-02],\n",
       "        [-1.96926274e-02],\n",
       "        [ 3.15791432e-01],\n",
       "        [ 5.25613480e-01],\n",
       "        [ 1.30994980e+00],\n",
       "        [-4.13031197e-01],\n",
       "        [ 8.29384918e-01],\n",
       "        [ 1.09815732e-01],\n",
       "        [-2.26940696e-02],\n",
       "        [-1.12900246e+00],\n",
       "        [-1.02028466e-01],\n",
       "        [-4.87884525e-01],\n",
       "        [-3.06098682e-01],\n",
       "        [-1.48316869e-01],\n",
       "        [-8.64989210e-01],\n",
       "        [-7.96988814e-01]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk = CCA(n_components=1)\n",
    "sk.fit(study1[0], study1[1])\n",
    "sk.transform(study1[0], study1[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "593ef910",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loadings = np.dot(study1[1].flatten(), projections[0][1]) / np.dot(projections[0][0], projections[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2: 0.0100, score: 0.4592\n",
      "l2: 0.1000, score: 0.4614\n",
      "l2: 1.0000, score: 0.1377\n",
      "l2: 10.0000, score: 0.0029\n",
      "Best l2: 0.1000, Best score: 0.4614\n",
      "Best L2: 0.1\n",
      "Best score: 0.4614104420916759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming `data` is a list of tuples: [(img1, beh1), (img2, beh2), ...]\n",
    "# Define a custom scorer if needed, but the `score` method should suffice\n",
    "\n",
    "search = GridSearchCA3(l2_values=[0.01, 0.1, 1.0, 10.0], theta=0.5, verbose=True)\n",
    "search.fit(study1)\n",
    "\n",
    "best_model = search.get_best_model()\n",
    "print(\"Best L2:\", search.get_best_l2())\n",
    "print(\"Best score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c618128",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x_loadings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x, x_p) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_p, x_p)\n\u001b[1;32m      6\u001b[0m y_loadings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y, y_p) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y_p, y_p)\n\u001b[0;32m----> 7\u001b[0m x_rotations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x, \u001b[43mpinv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_loadings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_loadings)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_loadings)\n",
      "File \u001b[0;32m~/envs/cca/lib/python3.10/site-packages/scipy/linalg/_basic.py:1607\u001b[0m, in \u001b[0;36mpinv\u001b[0;34m(a, atol, rtol, return_rank, check_finite)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03mCompute the (Moore-Penrose) pseudo-inverse of a matrix.\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m a \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[0;32m-> 1607\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43m_decomp_svd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m t \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m   1609\u001b[0m maxS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(s, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/envs/cca/lib/python3.10/site-packages/scipy/linalg/_decomp_svd.py:108\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    106\u001b[0m a1 \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m m, n \u001b[38;5;241m=\u001b[39m a1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# accommodate empty matrix\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected matrix"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import pinv as pinv2\n",
    "for (x, y), (x_p, y_p) in zip(ca3.data, ca3.projects_):\n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    x_loadings = np.dot(x, x_p) / np.dot(x_p, x_p)\n",
    "    y_loadings = np.dot(y, y_p) / np.dot(y_p, y_p)\n",
    "    x_rotations = np.dot(x, pinv2(np.dot(x_loadings.T, x )))\n",
    "    print(x_loadings)\n",
    "    print(y_loadings)\n",
    "    #print(y.shape[0])\n",
    "    #print(x_p.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a05cce5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m X1_proj_ca3, Y1_proj_ca3 \u001b[38;5;241m=\u001b[39m \u001b[43mtransfomed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprojections\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudy0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m display(scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mttest_ind(X1_proj_ca3\u001b[38;5;241m.\u001b[39mflatten(), X1_proj_cca\u001b[38;5;241m.\u001b[39mflatten() ))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats \n",
    "X1_proj_ca3, Y1_proj_ca3 = transfomed['projections']['study0']\n",
    "display(scipy.stats.ttest_ind(X1_proj_ca3.flatten(), X1_proj_cca.flatten() ))\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X1_proj_ca3, Y1_proj_ca3, c='teal')\n",
    "m, b = np.polyfit(X1_proj_ca3.flatten(), Y1_proj_ca3.flatten(), 1)\n",
    "plt.plot(X1_proj_ca3, m*X1_proj_ca3 + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {transfomed['correlations']['study0'][0]:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"CA3 projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X1_proj_cca, Y1_proj_cca, c='orange', label=f\"r = {sklearn_corr:.2f}\")\n",
    "m, b = np.polyfit(X1_proj_cca.flatten(), Y1_proj_cca.flatten(), 1)\n",
    "plt.plot(X1_proj_cca, m*X1_proj_cca + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {sklearn_corr:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"sklearn CCA projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
