{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d98ad7c",
   "metadata": {},
   "source": [
    "# CA3 model\n",
    "\n",
    "This notebook is for building out the CA3 model as previous implementation doesn't work.\n",
    "\n",
    "Formula is:\n",
    "$$\n",
    "min_{\\vec{w}_{x_1}, \\vec{w}_{x_2}, \\vec{w}_a, \\vec{w}_b} \\left( -\\vec{w}_{x_1}^T S_{xa} \\vec{w}_a - \\vec{w}_{x_2}^T S_{xb} \\vec{w}_b + \\sum_{i \\in \\{x_1, x_2, a, b\\}} \\frac{1}{2} \\lambda_i \\left( \\vec{w}_i^T S_{ii} \\vec{w}_i - 1 \\right) + \\theta_{rr}(\\vec{w}_{x_1}, \\vec{w}_{x_2}) \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0861b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemmr.generative_model import GEMMR\n",
    "from gemmr.estimators import SVDCCA\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d9b9d",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf757017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = GEMMR('cca', wx=1, wy=1, r_between=0.3)\n",
    "behavioural_data_study1, imging_data_study1 = model_definition.generate_data(n=200)\n",
    "behavioural_data_study2, imging_data_study2 = model_definition.generate_data(n=190)\n",
    "study1 = (imging_data_study1, behavioural_data_study1) \n",
    "study2 = (imging_data_study2, behavioural_data_study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9231845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2994662])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= SVDCCA().fit(imging_data_study1, behavioural_data_study1)\n",
    "test.corrs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8e3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum = test.transform(imging_data_study1, behavioural_data_study1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc9559",
   "metadata": {},
   "source": [
    "## Step 1: Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "523234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to demean data.\n",
    "\n",
    "    Parmeteres\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data to demean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        demeaned data\n",
    "    \"\"\"\n",
    "    return data - data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov(matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate \n",
    "    covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_1: np.ndarray\n",
    "        A matrix tht should \n",
    "        correspond to subject by \n",
    "        features\n",
    "    matrix_2: np.ndarray\n",
    "        A matrix that should \n",
    "        correspond to features by\n",
    "        feautres \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        array of cross covariance matrix\n",
    "    \"\"\"\n",
    "    return (matrix_1.T @ matrix_2) / matrix_1.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54813454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_able_to_process(study_pair: tuple, behav_data: np.ndarray, img_data: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check that data\n",
    "    is in correct format to be processed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     study_pair: tuple, \n",
    "         tuple of behavioural data \n",
    "         and imging data\n",
    "     behav_data: np.ndarray\n",
    "         array of behav_data \n",
    "     img_data: np.ndarray\n",
    "         array of img_data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool: boolean\n",
    "        bool of if failed or not\n",
    "    \"\"\"\n",
    "    if not isinstance(study_pair, (tuple, list)) or len(study_pair) != 2:\n",
    "        print(\"Given argument isn't a pair of datasets\")\n",
    "        return False\n",
    "    if not isinstance(behav_data, np.ndarray) or not isinstance(img_data, np.ndarray):\n",
    "        print(\"Data provided isn't a numpy array\")\n",
    "        return False\n",
    "    if behav_data.shape[0] == 0 or img_data.shape[0] == 0 or behav_data.shape[0] != img_data.shape[0]:\n",
    "        print(f\"Mismatch between ({behav_data.shape[0]} and {img_data.shape[0]})\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb76963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_matricies(*study_pairs) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates covariance matrices and auto covariance\n",
    "    matricies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_pairs: tuple\n",
    "        a tuple or list containing two numpy arrays:\n",
    "        (behavioural_data, imaging_data).\n",
    "        Assumes data is (subjects x features).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    covariance_results: dict\n",
    "        dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "    \"\"\"\n",
    "    covariance_results = {}\n",
    "    for idx, study_pair in enumerate(study_pairs):\n",
    "        img_data, behav_data  = study_pair\n",
    "        if not data_able_to_process(study_pair, behav_data, img_data):\n",
    "            continue\n",
    "        behav_data = mean_center(behav_data)\n",
    "        img_data = mean_center(img_data)\n",
    "        study_num = idx + 1\n",
    "        try:\n",
    "            covariance_results[f\"s_behav{study_num}_behav{study_num}\"] = cross_cov(behav_data, behav_data)\n",
    "            covariance_results[f\"s_img{study_num}_img{study_num}\"] = cross_cov(img_data, img_data)\n",
    "            covariance_results[f\"s_img{study_num}_behav{study_num}\"] = cross_cov(img_data, behav_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "            return None\n",
    "    return covariance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd37f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_mat = calculate_covariance_matricies(study1, study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36418b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x1a = cross_cov(behavioural_data_study1, imging_data_study1)\n",
    "s_x2b = cross_cov(behavioural_data_study2, imging_data_study2)\n",
    "s_x1x1 = cross_cov(behavioural_data_study1, behavioural_data_study1)\n",
    "s_x2x2 = cross_cov(behavioural_data_study2, behavioural_data_study2)\n",
    "s_aa = cross_cov(imging_data_study1, imging_data_study1)\n",
    "s_bb = cross_cov(imging_data_study2, imging_data_study2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040469c",
   "metadata": {},
   "source": [
    "## Step 2. Intialization of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9220d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_intialization(*weights) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Define a set of random starting \n",
    "    weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: tuple(int)\n",
    "        tuple of set amount\n",
    "        of int values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarrray\n",
    "        array of numpy values\n",
    "    \"\"\"\n",
    "    return np.random.randn(sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d329c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = {\n",
    "  '1': behavioural_data_study1.shape[1], \n",
    "   '2': behavioural_data_study2.shape[1],\n",
    "    '3': imging_data_study1.shape[1],\n",
    "    '4': imging_data_study2.shape[1]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6c66dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = weight_intialization(\n",
    "    behavioural_data_study1.shape[1], \n",
    "    behavioural_data_study2.shape[1],\n",
    "    imging_data_study1.shape[1],\n",
    "    imging_data_study2.shape[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b44fd",
   "metadata": {},
   "source": [
    "## Step 3. Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2db5333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx1_shape = s_x1x1.shape[0]\n",
    "dx2_shape = s_x2x2.shape[0]\n",
    "da_shape = s_aa.shape[0]\n",
    "db_shape = s_bb.shape[0]\n",
    "dx_shape = dx1_shape + dx2_shape\n",
    "dac_shape =  dx_shape + da_shape\n",
    "\n",
    "def get_dimensions(s_x1x1, s_x2x2, s_aa):\n",
    "    dx1_shape = s_x1x1.shape[0]\n",
    "    dx2_shape = s_x2x2.shape[0]\n",
    "    da_shape = s_aa.shape[0]\n",
    "    dx_shape =  dx1_shape + dx2_shape\n",
    "    return {\n",
    "        'dx1_shape': dx1_shape,\n",
    "        'dx_shape' : dx_shape,\n",
    "        'dac_shape':  dx_shape + da_shape\n",
    "    }\n",
    "\n",
    "def get_weights(weight_array, dx1_shape, dx_shape, dac_shape):\n",
    "    return {\n",
    "        \"wx1\": weight_array[:dx1_shape],\n",
    "        \"wx2\": weight_array[dx1_shape:dx_shape],\n",
    "        \"wa\": weight_array[dx_shape:dac_shape],\n",
    "        \"wb\": weight_array[dac_shape:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43838fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov_term(weight_beh, cov_mat, weight_img):\n",
    "    return -weight_img.T @ (cov_mat @ weight_beh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a0332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_term(weight, cov_mat):\n",
    "    return 0.5 * 1.0 * (weight.T @ (cov_mat @ weight) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeeb8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity_penality(theta_r, img_weight1, img_weight2):\n",
    "    return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79bec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(weights, s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r):\n",
    "    dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "    weights = get_weights(\n",
    "            weights, \n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])\n",
    "    term1 = cross_cov_term(weights['wx1'],s_x1a, weights['wa'])\n",
    "    term2 = cross_cov_term(weights['wx2'],s_x2b, weights['wb'])\n",
    "    reg_x1 = regularization_term(weights['wx1'], s_x1x1)\n",
    "    reg_x2 = regularization_term(weights['wx2'], s_x2x2)\n",
    "    reg_a = regularization_term(weights['wa'], s_aa)\n",
    "    reg_b = regularization_term(weights['wb'], s_bb)\n",
    "    theta_r = dissimilarity_penality(theta_r, weights['wa'], weights['wb'])\n",
    "    return term1 + term2 + reg_x1 + reg_x2 + reg_a + reg_b + theta_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9f1c9",
   "metadata": {},
   "source": [
    "## Step 4: Minimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3de1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "optimal_theta_r = None\n",
    "optimium_model = None\n",
    "\n",
    "for theta_r in np.logspace(-3, 2, 10):\n",
    "    weights_0 = weight_intialization(\n",
    "        behavioural_data_study1.shape[1], \n",
    "        behavioural_data_study2.shape[1],\n",
    "        imging_data_study1.shape[1],\n",
    "        imging_data_study2.shape[1])  # re-init each time\n",
    "    res = minimize(\n",
    "        objective_function,\n",
    "        weights_0,\n",
    "        args=(s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r),\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    if res.status !=0:\n",
    "        print(res.status)\n",
    "        continue\n",
    "\n",
    "\n",
    "    if res.fun < best_loss:\n",
    "        best_loss = res.fun\n",
    "        best_theta_r = theta_r\n",
    "        optimium_model = res\n",
    "\n",
    "print(f\"Best θ_r: {best_theta_r}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "weights = get_weights(optimium_model.x,\n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d427de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get projections (scores)\n",
    "scores_x1 = behavioural_data_study1 @ weights['wx1']\n",
    "scores_x2 = behavioural_data_study2 @ weights['wx2']\n",
    "scores_a  = imging_data_study1 @ weights['wa']\n",
    "scores_b  = imging_data_study2 @ weights['wb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11538425261898907)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.07889484464506155)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.00014508777456870848)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.corrcoef(scores_x1, scores_a)[0, 1])\n",
    "display(np.corrcoef(scores_x2, scores_b)[0, 1])\n",
    "display(np.linalg.norm(weights['wa'] - weights['wb']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfca62",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8fdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridSearchCA3:\n",
    "    def __init__(self, l2_values, theta=0, tol=1e-6, maxiter=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Custom grid search to find the best l2 value for the CA3 model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l2_values : list of float\n",
    "            The l2 regularization parameters to search over.\n",
    "        theta : float\n",
    "            The dissimilarity regularization parameter (shared across all models).\n",
    "        tol : float\n",
    "            Tolerance for optimization.\n",
    "        maxiter : int\n",
    "            Maximum number of optimization iterations.\n",
    "        verbose : bool\n",
    "            If True, print progress during search.\n",
    "        \"\"\"\n",
    "        self.l2_values = l2_values\n",
    "        self.theta = theta\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "        self.verbose = verbose\n",
    "        self.best_model_ = None\n",
    "        self.best_score_ = -np.inf\n",
    "        self.best_l2_ = None\n",
    "        self.all_results_ = []\n",
    "\n",
    "    def fit(self, *data_sets):\n",
    "        \"\"\"\n",
    "        Fit CA3 models with each l2 value and track the one with best score.\n",
    "        \"\"\"\n",
    "        for l2 in self.l2_values:\n",
    "            model = CA3(l2=l2, theta=self.theta, tol=self.tol, maxiter=self.maxiter)\n",
    "            model.fit(*data_sets)\n",
    "            score = model._score(*data_sets)\n",
    "\n",
    "            self.all_results_.append((l2, score))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"l2: {l2:.4f}, score: {score:.4f}\")\n",
    "\n",
    "            if score > self.best_score_:\n",
    "                self.best_score_ = score\n",
    "                self.best_model_ = model\n",
    "                self.best_l2_ = l2\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Best l2: {self.best_l2_:.4f}, Best score: {self.best_score_:.4f}\")\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model_\n",
    "\n",
    "    def get_best_l2(self):\n",
    "        return self.best_l2_\n",
    "\n",
    "    def get_all_results(self):\n",
    "        return self.all_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c072048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA3:\n",
    "    \"\"\"\n",
    "    CA3 class. \n",
    "    A class to do CA3 \n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    ca3 = CA3(l2=0.5, theta=1)\n",
    "    ca3.fit(study1, study2)\n",
    "    transformed = ca3.transform(study1, study2)\n",
    "    \"\"\"\n",
    "    def __init__(self, l2: float=1, theta: float=0, tol=1e-6, maxiter=500, normalise_weights=True):\n",
    "        self.l2_ = l2\n",
    "        self.theta_ = theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float('inf')\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        self.tol_ = tol\n",
    "        self.maxiter_ = maxiter\n",
    "        self.normalise_weights = normalise_weights\n",
    "        self.canonical_correlations_ = None\n",
    "\n",
    "    def fit(self, *data_sets: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Method to fit the CA3 model to a given \n",
    "        set of datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data \n",
    "            from an arbituray number of \n",
    "            datasets\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._calculate_covariance_matricies(*data_sets)\n",
    "        self._get_dimensions(*data_sets)\n",
    "        self._weight_intialization()\n",
    "        self._optimise()\n",
    "\n",
    "\n",
    "    def transform(self, *data_sets: tuple) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to transform data sets into canonical\n",
    "        projects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data \n",
    "            from an arbituray number of \n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the \n",
    "            projections of each dataset in \n",
    "            ndarry of n_components by n_samples\n",
    "        \"\"\"\n",
    "        assert self.weights_ is not None, \"Model must be fitted before transform can be called.\"\n",
    "        assert len(data_sets) == len(self.dims_), \"Model fitted with different number of datasets.\"\n",
    "        \n",
    "        projects =  [\n",
    "            np.stack([\n",
    "                self._normalise(img_data @ wx) if self.normalise_weights else img_data @ wx,\n",
    "                self._normalise(beh_data @ wb) if self.normalise_weights else beh_data @ wb\n",
    "            ], axis=0)\n",
    "            for (img_data, beh_data), (wx, wb) in zip(data_sets, self.weights_)\n",
    "        ]\n",
    "        \n",
    "        self.canonical_correlations_ = [np.corrcoef(data_sets[0], data_sets[1])[0,1] for data_sets in projects]   \n",
    "        return projects\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, *data_sets) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to fit a CA3 model and then transform\n",
    "        the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data \n",
    "            from an arbituray number of \n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the \n",
    "            projections of each dataset in \n",
    "            ndarry of n_components by n_samples.\n",
    "        \"\"\"\n",
    "        self.fit(*data_sets)\n",
    "        return self.transform(*data_sets)\n",
    "    \n",
    "    def calculate_canonical_correlations(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Method to obtain the canonical correlations.\n",
    "        Model must have been fitted and transfomed \n",
    "        before.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        canonical_correlations: list[float]\n",
    "            list of canonical correlations\n",
    "        \"\"\"\n",
    "        assert self.canonical_correlations_ is not None, \"Model must be fitted and transfomed before correlations can be returned\"\n",
    "        return self.canonical_correlations_\n",
    "    \n",
    "\n",
    "    def _weight_intialization(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to define a set of random starting \n",
    "        weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\" \n",
    "        init_weights = []\n",
    "\n",
    "        for idx, _ in enumerate(self.dims_):\n",
    "            s_xb = self.covariances_[f\"s_img{idx+1}_behav{idx+1}\"]\n",
    "            # Perform SVD on the cross-covariance matrix\n",
    "            try:\n",
    "                U, _, Vt = np.linalg.svd(s_xb, full_matrices=False)\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                raise RuntimeError(f\"SVD failed for dataset {idx+1}: {e}\")\n",
    "    \n",
    "\n",
    "            wx = U[:, 0]\n",
    "            wb = Vt.T[:, 0]    \n",
    "            s_xx = self.covariances_[f\"s_img{idx+1}_img{idx+1}\"]\n",
    "            s_bb = self.covariances_[f\"s_behav{idx+1}_behav{idx+1}\"]\n",
    "    \n",
    "            wx = wx / np.sqrt(wx.T @ s_xx @ wx)\n",
    "            wb = wb / np.sqrt(wb.T @ s_bb @ wb)\n",
    "    \n",
    "            init_weights.extend(wx)\n",
    "            init_weights.extend(wb)\n",
    "    \n",
    "        self.intial_weights_ = np.array(init_weights)\n",
    "\n",
    "    def _calculate_covariance_matricies(self, *data_sets) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance and auto covariance\n",
    "        matricies\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "    \n",
    "        \"\"\"\n",
    "        for idx, study_pair in enumerate(data_sets):\n",
    "            img_data, behav_data  = study_pair\n",
    "            self._data_able_to_process(study_pair)\n",
    "            behav_data = self._normalise(behav_data)\n",
    "            img_data = self._normalise(img_data)\n",
    "            study_num = idx + 1\n",
    "            try:\n",
    "                self.covariances_[f\"s_behav{study_num}_behav{study_num}\"] = self._create_covariance_matrix(behav_data, behav_data)\n",
    "                self.covariances_[f\"s_img{study_num}_img{study_num}\"] = self._create_covariance_matrix(img_data, img_data)\n",
    "                self.covariances_[f\"s_img{study_num}_behav{study_num}\"] = self._create_covariance_matrix(img_data, behav_data)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, study_pair: tuple) -> bool:\n",
    "        \"\"\"\n",
    "        Method to check that data\n",
    "        is in correct format to be processed\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple, \n",
    "             tuple of behavioural data \n",
    "             and imging data\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        assert isinstance(study_pair, (tuple, list)) and len(study_pair) == 2, \"Given argument isn't a pair of datasets\"\n",
    "        assert isinstance(study_pair[0], np.ndarray) or not isinstance(study_pair[1], np.ndarray), \"Data provided ins't numpy array\"\n",
    "        assert (study_pair[0].shape[0] != 0) and (study_pair[1].shape[0] != 0), \\\n",
    "             \"Study pairs contains not data\"\n",
    "        assert study_pair[0].shape[0] == study_pair[1].shape[0], \\\n",
    "              f\"Mismatch between ({study_pair[0].shape[0]} and {study_pair[1].shape[0]})\"\n",
    "            \n",
    "    def _optimise(self):\n",
    "        \"\"\"\n",
    "        Method to minimise the \n",
    "        objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        None \n",
    "        \"\"\"\n",
    "        model = minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            options={'gtol': self.tol_, \"maxiter\": self.maxiter_},\n",
    "            args=(self.covariances_, self.theta_, self.l2_),\n",
    "        )\n",
    "        self.best_loss = model.fun\n",
    "        self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "    \n",
    "    def _get_dimensions(self, *data_sets) -> None:\n",
    "        \"\"\"\n",
    "        Method to check the number of dimensions\n",
    "        that the \n",
    "        \"\"\"\n",
    "        self.dims_ = [(behav.shape[1], img.shape[1]) for behav, img in data_sets]\n",
    "    \n",
    "    def _split_weights(self, weights: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector weights into individual vectors\n",
    "        for each x and b dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            flatten numpy array\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        split_weights: list[np.ndarray]\n",
    "            list of weights split\n",
    "            wx and wb \n",
    "\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        split_weights = []\n",
    "        for img_dim, behav_dim in self.dims_:\n",
    "            wx = weights[offset:offset + img_dim]\n",
    "            offset += img_dim  \n",
    "            wb = weights[offset:offset + behav_dim]\n",
    "            offset += behav_dim\n",
    "            split_weights.append((wx, wb))\n",
    "        return split_weights \n",
    "    \n",
    "    def _objective_function(self, \n",
    "                            weights: np.ndarray, \n",
    "                            covariances: dict, \n",
    "                            theta: float, \n",
    "                            l2: float) -> float:\n",
    "        \"\"\"\n",
    "        Objective function of the CA3 class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            weights \n",
    "        covariances: dict\n",
    "            dict of cross/auto covariance\n",
    "            matricies\n",
    "        theta: float\n",
    "            theta penality\n",
    "        l2: float\n",
    "            regularization penailty\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        total_loss: float\n",
    "           total loss of the objective function\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        weights_ = self._split_weights(weights)\n",
    "        for idx, (wx, wb) in enumerate(weights_):\n",
    "            s_xb = covariances[f\"s_img{idx+1}_behav{idx+1}\"]\n",
    "            s_xx = covariances[f\"s_img{idx+1}_img{idx+1}\"]\n",
    "            s_bb = covariances[f\"s_behav{idx+1}_behav{idx+1}\"]\n",
    "            total_loss += self._cross_cov_term(wb, s_xb, wx) \n",
    "            total_loss += self._regularization_term(wx, s_xx, l2)\n",
    "            total_loss += self._regularization_term(wb, s_bb, l2)\n",
    "    \n",
    "        # Similarity penalty across imaging weights\n",
    "        if theta > 0 and len(weights_) > 1:\n",
    "            for img_data in range(len(weights_)):\n",
    "                for next_img_data in range(img_data + 1, len(weights_)):\n",
    "                    total_loss += self._dissimilarity_penality(theta, weights_[img_data][0], weights_[next_img_data][0])\n",
    "    \n",
    "        return total_loss\n",
    "    \n",
    "    def _create_covariance_matrix(self, matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cross-auto\n",
    "        covariance matrix\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should \n",
    "            correspond to subject by \n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should \n",
    "            correspond to features by\n",
    "            feautres \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0] \n",
    "\n",
    "    \n",
    "    def _normalise(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to normalise data.\n",
    "    \n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        dmean = data - data.mean(axis=0)\n",
    "        std = data.std(axis=0, ddof=1)\n",
    "        std = np.where(std == 0.0, 1.0, std)\n",
    "        return dmean / std\n",
    "\n",
    "    def _cross_cov_term(self, weight_beh: np.ndarray, cov_mat: np.ndarray, weight_img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to calculate the cross covarance term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_beh: np.ndarray\n",
    "            set of weights for wb\n",
    "        cov_mat: np.ndarray\n",
    "             covariance matrix for \n",
    "             wx wb\n",
    "        weight_img: np.ndarray\n",
    "            set of weights for wx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: np.array\n",
    "            cross covariance term\n",
    "        \"\"\"\n",
    "        return -weight_img.T @ (cov_mat @ weight_beh)\n",
    "\n",
    "    def _regularization_term(self, weight: np.ndarray, cov_mat: np.ndarray, lambda_i: float) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the regularization term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight: np.ndarray\n",
    "            set of weights\n",
    "        cov_mat: np.ndarray\n",
    "            auto covariance matrix\n",
    "        lambda_i: float\n",
    "            regularization parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            regularization term of the objective function\n",
    "        \n",
    "        \"\"\"\n",
    "        return 0.5 * lambda_i * (weight.T @ (cov_mat @ weight) - 1)\n",
    "\n",
    "    def _dissimilarity_penality(self, theta_r: float, img_weight1: np.ndarray, img_weight2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Method to return dissimilarity penality\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        theta_r: float\n",
    "           theta penality. \n",
    "        img_weight1: np.ndarray\n",
    "            weights of imaging data \n",
    "        img_weight2: np.ndarray\n",
    "            weights of second imaging\n",
    "            data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            dissimilarity penality\n",
    "        \"\"\"\n",
    "        return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)\n",
    "    \n",
    "    def _score(self, *data_sets: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Method used to evaluate model performance. \n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data \n",
    "            from an arbituray number of \n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            mean of correlation \n",
    "            values across datasets\n",
    "\n",
    "        \"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before scoring.\")\n",
    "\n",
    "        result = self.transform(*data_sets)\n",
    "        correlations = self.calculate_canonical_correlations()\n",
    "        all_corrs = [corr[0] for corr in correlations.values()]\n",
    "        return np.mean(all_corrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2: 0.0100, score: 0.4592\n",
      "l2: 0.1000, score: 0.4614\n",
      "l2: 1.0000, score: 0.1377\n",
      "l2: 10.0000, score: 0.0029\n",
      "Best l2: 0.1000, Best score: 0.4614\n",
      "Best L2: 0.1\n",
      "Best score: 0.4614104420916759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming `data` is a list of tuples: [(img1, beh1), (img2, beh2), ...]\n",
    "# Define a custom scorer if needed, but the `score` method should suffice\n",
    "\n",
    "search = GridSearchCA3(l2_values=[0.01, 0.1, 1.0, 10.0], theta=0.5, verbose=True)\n",
    "search.fit(study1)\n",
    "\n",
    "best_model = search.get_best_model()\n",
    "print(\"Best L2:\", search.get_best_l2())\n",
    "print(\"Best score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5254108",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca3 = CA3(l2=0.0)\n",
    "#ca3.fit(study1, study2)\n",
    "transfomed = ca3.fit_transform(study1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ecb8685f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.01541487,  1.54195171, -0.82720464,  0.44531447, -1.10737123,\n",
       "          1.89149745,  1.42566874, -2.75281769,  0.14666389, -0.83988258,\n",
       "         -2.65004166, -2.08228398, -0.63950613,  0.30895288,  0.11925364,\n",
       "          0.82569118, -0.71717044, -0.16163601,  0.5003831 , -1.11803469,\n",
       "          1.20368527,  0.66843183, -0.35736554,  1.58343172,  0.46523375,\n",
       "          1.67110053,  0.30132398,  1.01197457, -1.33225903,  0.06807949,\n",
       "          1.78555422, -2.02771024, -0.67956878,  0.43363717,  0.66626076,\n",
       "         -0.77252174, -0.06647668, -0.60448251, -0.70095669,  0.39902183,\n",
       "         -0.84551603,  0.16606879,  1.60429135, -1.10802307, -0.58143167,\n",
       "         -0.43654064,  1.46537596, -0.69549266,  1.09734877, -1.17342806,\n",
       "         -0.8643437 , -1.59824084,  0.07726928,  1.56111942,  0.00839318,\n",
       "         -2.54446766,  1.18810269, -0.32386299,  0.28368249,  1.07915144,\n",
       "         -0.41293849,  0.39427985, -0.80754778,  0.14618212, -0.25924027,\n",
       "          0.35763027, -0.79107642,  0.65625616, -0.05654809, -0.16057977,\n",
       "          0.23405613, -0.26906241,  0.061859  ,  1.0761046 ,  0.90025647,\n",
       "         -0.39463228,  0.04994881, -0.15206871, -0.34418183,  1.24334278,\n",
       "          0.67936147, -0.75621158, -0.40172744,  0.05350452,  0.67910661,\n",
       "         -0.67397189,  0.1734515 , -1.34707105, -0.68564499, -0.6414355 ,\n",
       "          1.35505449, -1.42996138,  2.47946888,  0.02960991,  1.58771893,\n",
       "          0.28422967,  1.42700327,  0.10511223, -0.63466101,  1.72080487,\n",
       "          1.52526046,  0.42669487,  0.95364891,  1.13685149,  1.8407412 ,\n",
       "         -0.28268495,  1.99979537,  0.95293169,  0.48010578, -1.06990275,\n",
       "         -0.24213685, -1.53574162, -1.06319032, -0.66923277, -1.01017987,\n",
       "          0.91793774,  0.1951623 , -1.23109334,  0.11476321, -0.89561956,\n",
       "         -1.10368903, -1.33624586,  0.31643035,  0.45866262, -0.88309246,\n",
       "         -0.98245925,  0.7378669 , -0.89117149, -0.18350465, -1.11716894,\n",
       "          0.76830738,  0.34941218, -0.99211452,  0.00763403, -0.79963254,\n",
       "         -1.18747478,  0.00859101,  0.04986792,  0.56939361,  0.0528141 ,\n",
       "          0.33255984, -1.71578842,  0.4906162 , -0.95487205, -0.88607238,\n",
       "          0.27725588,  1.47761322, -1.90929015, -0.23226767, -1.34841853,\n",
       "          0.61649185, -1.00014382, -1.63228831, -1.44725115, -0.08243009,\n",
       "          0.44263526,  0.02016962,  0.18777505,  1.02322023,  0.81777179,\n",
       "          0.27469782,  1.39203612,  1.24970039,  1.57583985, -0.68112299,\n",
       "          0.93452242, -1.29883945, -1.30574978,  1.40852733, -0.00401635,\n",
       "          0.67803852,  0.34138938,  0.26570701,  2.26924129, -0.3451187 ,\n",
       "         -0.14926814,  0.3319601 ,  1.12134256,  1.17235823, -1.04504226,\n",
       "          0.84171091, -1.31406818,  0.1361562 ,  0.11520448,  0.81606517,\n",
       "          0.14510704, -0.64547085,  1.08476897,  0.96015495, -0.46650856,\n",
       "          0.41995845, -0.36974526,  0.13822302,  0.11742655, -0.12051438,\n",
       "         -1.05877962,  0.44676004, -1.10761981,  0.54584297,  0.41965591],\n",
       "        [-0.81932844, -1.46931162, -0.58025212,  0.91074549,  0.21360508,\n",
       "         -0.79582413,  0.24697643,  0.14013592, -0.22890748,  0.55143523,\n",
       "          1.12424018, -1.84355348, -0.15555227,  0.45043373, -0.71558393,\n",
       "          1.76910938, -0.158862  ,  0.10746215,  1.03397436, -2.17104077,\n",
       "         -1.14477645,  1.07073041,  0.93595308,  1.46836575,  0.60622542,\n",
       "          1.54224276,  0.25460793, -1.60277439, -0.11859678,  0.48502196,\n",
       "          0.25707328, -0.65543373,  1.29245424,  0.95981261,  2.00081557,\n",
       "         -1.28324248, -0.14917474, -0.90702654, -0.25475201,  1.85612528,\n",
       "          0.06529555,  0.9520269 ,  1.55216186,  0.30151177, -1.50312751,\n",
       "         -0.44607646,  0.61471772, -0.51252768, -0.21084037, -0.18427927,\n",
       "          1.55559438, -1.19743974, -1.58812434,  0.95017647,  0.24936762,\n",
       "         -0.81589229, -0.86357237, -0.64801741, -1.16437796, -0.02217276,\n",
       "         -1.16579501, -0.32575792,  0.0291106 ,  0.58218147, -0.35479262,\n",
       "         -1.94364348, -1.45200717,  1.19484125,  0.87494059,  0.08793403,\n",
       "          0.70523038,  1.00716755,  0.24562044, -0.89940266,  1.05170814,\n",
       "         -0.74481915, -0.77722215, -0.59425089, -1.38274577, -0.24346803,\n",
       "          0.81320794, -0.42823366,  0.23839461,  0.45909185,  1.82123152,\n",
       "          0.72782689,  0.9464384 ,  1.53652113, -0.84475498,  0.36863778,\n",
       "          1.34104315,  0.02821548,  1.12287221, -0.3366224 , -1.19354911,\n",
       "          0.03028992,  0.28596047,  0.01604519,  1.0444693 ,  1.3270555 ,\n",
       "          0.55715214,  0.0176819 , -1.44807535,  1.85497895,  1.73785179,\n",
       "         -0.08299204, -0.95684752,  0.95335011,  2.25987   ,  0.46379956,\n",
       "         -0.09230935, -0.2284983 , -0.29087358, -1.27591894, -0.93270767,\n",
       "          0.07185756, -1.8066457 ,  0.93252638, -0.16408439, -1.16126718,\n",
       "         -2.43612748, -0.21549533, -1.47322175, -0.18685793,  1.81741977,\n",
       "         -0.63897251, -0.13868385, -0.98947726, -2.27152562, -0.72564123,\n",
       "          0.02538922,  0.91308476,  2.081383  , -1.13135119,  0.59446263,\n",
       "         -0.52751943, -0.80980496, -0.16249158, -0.91820168, -0.29420342,\n",
       "          0.95972899, -1.65408161, -0.00662157, -1.17024126, -1.05487437,\n",
       "         -0.63451697,  0.68673719, -2.37620101,  1.46162002,  0.20546121,\n",
       "         -0.06735373,  0.33697219, -0.07599334, -0.02374754,  1.49195094,\n",
       "          1.3288344 ,  0.19258726, -1.37957906,  0.45779323, -0.83594205,\n",
       "         -0.20892728, -0.84510384,  0.22740467, -1.3647627 ,  0.19660747,\n",
       "         -0.02539607, -1.32891848, -0.76056938,  0.14777315, -1.05770783,\n",
       "         -0.56262464, -0.67753878,  1.90960245,  0.49527465,  0.16370671,\n",
       "          0.42622464,  2.65444167,  0.66699599,  0.36662632,  0.47280227,\n",
       "          0.25917767,  0.68773414, -0.6003044 ,  0.28278814,  0.61195026,\n",
       "          1.54808377,  0.4580286 , -0.25749585, -1.74535967,  1.1985074 ,\n",
       "         -0.45162525, -0.12941086, -0.92567948,  0.57325387,  0.14767986,\n",
       "          0.27614848,  0.03162531, -0.47566582, -0.86372922,  1.20790886]])]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfomed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a56c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.29946620246301203)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correl = ca3.calculate_canonical_correlations()\n",
    "correl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e060720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation from sklearn CCA: 0.29946620246301203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "test_sck = CCA(n_components=1)\n",
    "test_sck.fit(study1[0], study1[1])\n",
    "X1_proj_cca, Y1_proj_cca = test_sck.transform(study1[0], study1[1])\n",
    "sklearn_corr = np.corrcoef(X1_proj_cca.ravel(), Y1_proj_cca.ravel())[0,1]\n",
    "print(f\"Correlation from sklearn CCA: {sklearn_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a05cce5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m X1_proj_ca3, Y1_proj_ca3 \u001b[38;5;241m=\u001b[39m \u001b[43mtransfomed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprojections\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudy0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m display(scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mttest_ind(X1_proj_ca3\u001b[38;5;241m.\u001b[39mflatten(), X1_proj_cca\u001b[38;5;241m.\u001b[39mflatten() ))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats \n",
    "X1_proj_ca3, Y1_proj_ca3 = transfomed['projections']['study0']\n",
    "display(scipy.stats.ttest_ind(X1_proj_ca3.flatten(), X1_proj_cca.flatten() ))\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X1_proj_ca3, Y1_proj_ca3, c='teal')\n",
    "m, b = np.polyfit(X1_proj_ca3.flatten(), Y1_proj_ca3.flatten(), 1)\n",
    "plt.plot(X1_proj_ca3, m*X1_proj_ca3 + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {transfomed['correlations']['study0'][0]:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"CA3 projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X1_proj_cca, Y1_proj_cca, c='orange', label=f\"r = {sklearn_corr:.2f}\")\n",
    "m, b = np.polyfit(X1_proj_cca.flatten(), Y1_proj_cca.flatten(), 1)\n",
    "plt.plot(X1_proj_cca, m*X1_proj_cca + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {sklearn_corr:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"sklearn CCA projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
