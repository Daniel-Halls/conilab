{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d98ad7c",
   "metadata": {},
   "source": [
    "# CA3 model\n",
    "\n",
    "This notebook is for building out the CA3 model as previous implementation doesn't work.\n",
    "\n",
    "Formula is:\n",
    "$$\n",
    "min_{\\vec{w}_{x_1}, \\vec{w}_{x_2}, \\vec{w}_a, \\vec{w}_b} \\left( -\\vec{w}_{x_1}^T S_{xa} \\vec{w}_a - \\vec{w}_{x_2}^T S_{xb} \\vec{w}_b + \\sum_{i \\in \\{x_1, x_2, a, b\\}} \\frac{1}{2} \\lambda_i \\left( \\vec{w}_i^T S_{ii} \\vec{w}_i - 1 \\right) + \\theta_{rr}(\\vec{w}_{x_1}, \\vec{w}_{x_2}) \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0861b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszdjh3/envs/c3a/lib/python3.13/site-packages/gemmr/sample_size/linear_model.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    }
   ],
   "source": [
    "from gemmr.generative_model import GEMMR\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from gemmr.estimators import SVDCCA\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d9b9d",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf757017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = GEMMR('cca', wx=1, wy=1, r_between=0.3)\n",
    "behavioural_data_study1, imging_data_study1 = model_definition.generate_data(n=2000)\n",
    "behavioural_data_study2, imging_data_study2 = model_definition.generate_data(n=190)\n",
    "study1 = (imging_data_study1, behavioural_data_study1) \n",
    "study2 = (imging_data_study2, behavioural_data_study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9231845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29587629])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= SVDCCA().fit(imging_data_study1, behavioural_data_study1)\n",
    "test.corrs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e8e3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum = test.transform(imging_data_study1, behavioural_data_study1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc9559",
   "metadata": {},
   "source": [
    "## Step 1: Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "523234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to demean data.\n",
    "\n",
    "    Parmeteres\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data to demean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        demeaned data\n",
    "    \"\"\"\n",
    "    return data - data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov(matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate \n",
    "    covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_1: np.ndarray\n",
    "        A matrix tht should \n",
    "        correspond to subject by \n",
    "        features\n",
    "    matrix_2: np.ndarray\n",
    "        A matrix that should \n",
    "        correspond to features by\n",
    "        feautres \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        array of cross covariance matrix\n",
    "    \"\"\"\n",
    "    return (matrix_1.T @ matrix_2) / matrix_1.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_able_to_process(study_pair: tuple, behav_data: np.ndarray, X_dat: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check that data\n",
    "    is in correct format to be processed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     study_pair: tuple, \n",
    "         tuple of behavioural data \n",
    "         and imging data\n",
    "     behav_data: np.ndarray\n",
    "         array of behav_data \n",
    "     X_dat: np.ndarray\n",
    "         array of X_dat\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool: boolean\n",
    "        bool of if failed or not\n",
    "    \"\"\"\n",
    "    if not isinstance(study_pair, (tuple, list)) or len(study_pair) != 2:\n",
    "        print(\"Given argument isn't a pair of datasets\")\n",
    "        return False\n",
    "    if not isinstance(behav_data, np.ndarray) or not isinstance(X_dat, np.ndarray):\n",
    "        print(\"Data provided isn't a numpy array\")\n",
    "        return False\n",
    "    if behav_data.shape[0] == 0 or X_dat.shape[0] == 0 or behav_data.shape[0] != X_dat.shape[0]:\n",
    "        print(f\"Mismatch between ({behav_data.shape[0]} and {X_dat.shape[0]})\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb76963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_matricies(*study_pairs) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates covariance matrices and auto covariance\n",
    "    matricies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_pairs: tuple\n",
    "        a tuple or list containing two numpy arrays:\n",
    "        (behavioural_data, imaging_data).\n",
    "        Assumes data is (subjects x features).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    covariance_results: dict\n",
    "        dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "    \"\"\"\n",
    "    covariance_results = {}\n",
    "    for idx, study_pair in enumerate(study_pairs):\n",
    "        X_dat, behav_data  = study_pair\n",
    "        if not data_able_to_process(study_pair, behav_data, X_dat):\n",
    "            continue\n",
    "        behav_data = mean_center(behav_data)\n",
    "        X_dat = mean_center(X_dat)\n",
    "        study_num = idx + 1\n",
    "        try:\n",
    "            covariance_results[f\"s_behav{study_num}_behav{study_num}\"] = cross_cov(behav_data, behav_data)\n",
    "            covariance_results[f\"s_img{study_num}_img{study_num}\"] = cross_cov(X_dat, X_dat)\n",
    "            covariance_results[f\"s_img{study_num}_behav{study_num}\"] = cross_cov(X_dat, behav_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "            return None\n",
    "    return covariance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd37f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_mat = calculate_covariance_matricies(study1, study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36418b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x1a = cross_cov(behavioural_data_study1, imging_data_study1)\n",
    "s_x2b = cross_cov(behavioural_data_study2, imging_data_study2)\n",
    "s_x1x1 = cross_cov(behavioural_data_study1, behavioural_data_study1)\n",
    "s_x2x2 = cross_cov(behavioural_data_study2, behavioural_data_study2)\n",
    "s_aa = cross_cov(imging_data_study1, imging_data_study1)\n",
    "s_bb = cross_cov(imging_data_study2, imging_data_study2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040469c",
   "metadata": {},
   "source": [
    "## Step 2. Intialization of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9220d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_intialization(*weights) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Define a set of random starting \n",
    "    weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: tuple(int)\n",
    "        tuple of set amount\n",
    "        of int values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarrray\n",
    "        array of numpy values\n",
    "    \"\"\"\n",
    "    return np.random.randn(sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d329c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = {\n",
    "  '1': behavioural_data_study1.shape[1], \n",
    "   '2': behavioural_data_study2.shape[1],\n",
    "    '3': imging_data_study1.shape[1],\n",
    "    '4': imging_data_study2.shape[1]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6c66dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = weight_intialization(\n",
    "    behavioural_data_study1.shape[1], \n",
    "    behavioural_data_study2.shape[1],\n",
    "    imging_data_study1.shape[1],\n",
    "    imging_data_study2.shape[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b44fd",
   "metadata": {},
   "source": [
    "## Step 3. Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2db5333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx1_shape = s_x1x1.shape[0]\n",
    "dx2_shape = s_x2x2.shape[0]\n",
    "da_shape = s_aa.shape[0]\n",
    "db_shape = s_bb.shape[0]\n",
    "dx_shape = dx1_shape + dx2_shape\n",
    "dac_shape =  dx_shape + da_shape\n",
    "\n",
    "def get_dimensions(s_x1x1, s_x2x2, s_aa):\n",
    "    dx1_shape = s_x1x1.shape[0]\n",
    "    dx2_shape = s_x2x2.shape[0]\n",
    "    da_shape = s_aa.shape[0]\n",
    "    dx_shape =  dx1_shape + dx2_shape\n",
    "    return {\n",
    "        'dx1_shape': dx1_shape,\n",
    "        'dx_shape' : dx_shape,\n",
    "        'dac_shape':  dx_shape + da_shape\n",
    "    }\n",
    "\n",
    "def get_weights(weight_array, dx1_shape, dx_shape, dac_shape):\n",
    "    return {\n",
    "        \"wx1\": weight_array[:dx1_shape],\n",
    "        \"wx2\": weight_array[dx1_shape:dx_shape],\n",
    "        \"wa\": weight_array[dx_shape:dac_shape],\n",
    "        \"wb\": weight_array[dac_shape:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43838fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov_term(weight_beh, cov_mat, weight_img):\n",
    "    return -weight_img.T @ (cov_mat @ weight_beh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a0332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_term(weight, cov_mat):\n",
    "    return 0.5 * 1.0 * (weight.T @ (cov_mat @ weight) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeeb8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity_penality(theta_r, img_weight1, img_weight2):\n",
    "    return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79bec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(weights, s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r):\n",
    "    dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "    weights = get_weights(\n",
    "            weights, \n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])\n",
    "    term1 = cross_cov_term(weights['wx1'],s_x1a, weights['wa'])\n",
    "    term2 = cross_cov_term(weights['wx2'],s_x2b, weights['wb'])\n",
    "    reg_x1 = regularization_term(weights['wx1'], s_x1x1)\n",
    "    reg_x2 = regularization_term(weights['wx2'], s_x2x2)\n",
    "    reg_a = regularization_term(weights['wa'], s_aa)\n",
    "    reg_b = regularization_term(weights['wb'], s_bb)\n",
    "    theta_r = dissimilarity_penality(theta_r, weights['wa'], weights['wb'])\n",
    "    return term1 + term2 + reg_x1 + reg_x2 + reg_a + reg_b + theta_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9f1c9",
   "metadata": {},
   "source": [
    "## Step 4: Minimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3de1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "optimal_theta_r = None\n",
    "optimium_model = None\n",
    "\n",
    "for theta_r in np.logspace(-3, 2, 10):\n",
    "    weights_0 = weight_intialization(\n",
    "        behavioural_data_study1.shape[1], \n",
    "        behavioural_data_study2.shape[1],\n",
    "        imging_data_study1.shape[1],\n",
    "        imging_data_study2.shape[1])  # re-init each time\n",
    "    res = minimize(\n",
    "        objective_function,\n",
    "        weights_0,\n",
    "        args=(s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r),\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    if res.status !=0:\n",
    "        print(res.status)\n",
    "        continue\n",
    "\n",
    "\n",
    "    if res.fun < best_loss:\n",
    "        best_loss = res.fun\n",
    "        best_theta_r = theta_r\n",
    "        optimium_model = res\n",
    "\n",
    "print(f\"Best θ_r: {best_theta_r}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "weights = get_weights(optimium_model.x,\n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d427de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get projections (scores)\n",
    "scores_x1 = behavioural_data_study1 @ weights['wx1']\n",
    "scores_x2 = behavioural_data_study2 @ weights['wx2']\n",
    "scores_a  = imging_data_study1 @ weights['wa']\n",
    "scores_b  = imging_data_study2 @ weights['wb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11538425261898907)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.07889484464506155)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.00014508777456870848)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.corrcoef(scores_x1, scores_a)[0, 1])\n",
    "display(np.corrcoef(scores_x2, scores_b)[0, 1])\n",
    "display(np.linalg.norm(weights['wa'] - weights['wb']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfca62",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736d52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class C3A:\n",
    "    \"\"\"\n",
    "    C3A class.\n",
    "    A class to do C3A\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    c3a = C3A(l2=0.5, theta=1)\n",
    "    c3a.fit(study1, study2)\n",
    "    transformed = c3a.transform(study1, study2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2: float = 1,\n",
    "        theta: float = 0,\n",
    "        tol=1e-6,\n",
    "        maxiter=500,\n",
    "    ):\n",
    "        self.l2_ = l2\n",
    "        self.theta_ = theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        self.tol_ = tol\n",
    "        self.maxiter_ = maxiter\n",
    "        self.canonical_correlations_ = None\n",
    "        self.projections_ = None\n",
    "\n",
    "    def fit(self, *data_sets: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Method to fit the CA3 model to a given\n",
    "        set of datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        data_sets = self._normalise_input_data(*data_sets)\n",
    "        self._calculate_covariance_matricies(*data_sets)\n",
    "        self._get_dimensions(*data_sets)\n",
    "        self._weight_intialization()\n",
    "        self._optimise()\n",
    "\n",
    "    def transform(self, *data_sets: tuple) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to transform data sets into canonical\n",
    "        projects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.weights_ is not None\n",
    "        ), \"Model must be fitted before transform can be called.\"\n",
    "        assert len(data_sets) == len(\n",
    "            self.dims_\n",
    "        ), \"Model fitted with different number of datasets.\"\n",
    "        data_sets = self._normalise_input_data(*data_sets)\n",
    "        self.projections_ = [\n",
    "            np.stack(\n",
    "                [\n",
    "                    self._normalise(X_data @ wx),\n",
    "                    self._normalise(Y_data @ wb),\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "            for (X_data, Y_data), (wx, wb) in zip(data_sets, self.weights_)\n",
    "        ]\n",
    "\n",
    "        self.canonical_correlations_ = [\n",
    "            np.corrcoef(data_sets[0], data_sets[1])[0, 1]\n",
    "            for data_sets in self.projections_\n",
    "        ]\n",
    "        return self.projections_\n",
    "\n",
    "    def fit_transform(self, *data_sets) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to fit a CA3 model and then transform\n",
    "        the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples.\n",
    "        \"\"\"\n",
    "        self.fit(*data_sets)\n",
    "        return self.transform(*data_sets)\n",
    "\n",
    "    def calculate_canonical_correlations(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Method to obtain the canonical correlations.\n",
    "        Model must have been fitted and transfomed\n",
    "        before.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        canonical_correlations: list[float]\n",
    "            list of canonical correlations\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.canonical_correlations_ is not None\n",
    "        ), \"Model must be fitted and transfomed before correlations can be returned\"\n",
    "        return self.canonical_correlations_\n",
    "\n",
    "    def compute_loadings(\n",
    "        self, *data_sets: tuple\n",
    "    ) -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Computes canonical loadings for each study.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (X, Y) pairs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loadings: list of tuples\n",
    "            Each tuple contains (X, Y), i.e., correlations between\n",
    "            original features and their respective canonical variates.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.projections_ is not None\n",
    "        ), \"Model must be fitted and transfomed before computing loadings.\"\n",
    "        data_sets = self._normalise_input_data(*data_sets)\n",
    "        return [\n",
    "            (\n",
    "                np.corrcoef(X_data.T, x_proj, rowvar=True)[:-1, -1],\n",
    "                np.corrcoef(Y_data.T, y_proj, rowvar=True)[:-1, -1],\n",
    "            )\n",
    "            for (X_data, Y_data), (x_proj, y_proj) in zip(data_sets, self.projections_)\n",
    "        ]\n",
    "    def _normalise_input_data(self, *data_sets) -> tuple:\n",
    "        \"\"\"\n",
    "        Normalise input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (X, Y) pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data_set: tuple\n",
    "            tuple of normalised data\n",
    "        \"\"\"\n",
    "        return tuple((self._normalise(X), self._normalise(Y)) for X, Y in data_sets)\n",
    "    \n",
    "    def _weight_intialization(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to define a set of random starting\n",
    "        weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\"\n",
    "        init_weights = []\n",
    "\n",
    "        for idx, _ in enumerate(self.dims_):\n",
    "            s_xb = self.covariances_[f\"s_X{idx+1}_Y{idx+1}\"]\n",
    "            # Perform SVD on the cross-covariance matrix\n",
    "            try:\n",
    "                U, _, Vt = np.linalg.svd(s_xb, full_matrices=False)\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                raise RuntimeError(f\"SVD failed for dataset {idx+1}: {e}\")\n",
    "\n",
    "            wx = U[:, 0]\n",
    "            wb = Vt.T[:, 0]\n",
    "            s_xx = self.covariances_[f\"s_X{idx+1}_X{idx+1}\"]\n",
    "            s_bb = self.covariances_[f\"s_Y{idx+1}_Y{idx+1}\"]\n",
    "\n",
    "            wx = wx / np.sqrt(wx.T @ s_xx @ wx + 1e-8)\n",
    "            wb = wb / np.sqrt(wb.T @ s_bb @ wb + 1e-8)\n",
    "\n",
    "            init_weights.extend(wx)\n",
    "            init_weights.extend(wb)\n",
    "\n",
    "        self.intial_weights_ = np.array(init_weights)\n",
    "\n",
    "    def _calculate_covariance_matricies(self, *data_sets) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance and auto covariance\n",
    "        matricies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "        \"\"\"\n",
    "        for idx, study_pair in enumerate(data_sets):\n",
    "            X_data, Y_data = study_pair\n",
    "            self._data_able_to_process(study_pair)\n",
    "            study_num = idx + 1\n",
    "            try:\n",
    "                self.covariances_[f\"s_Y{study_num}_Y{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(Y_data, Y_data)\n",
    "                )\n",
    "                self.covariances_[f\"s_X{study_num}_X{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(X_data, X_data)\n",
    "                )\n",
    "                self.covariances_[f\"s_X{study_num}_Y{study_num}\"] = (\n",
    "                    self._create_covariance_matrix(X_data, Y_data)\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, study_pair: tuple) -> bool:\n",
    "        \"\"\"\n",
    "        Method to check that data\n",
    "        is in correct format to be processed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple,\n",
    "             tuple of behavioural data\n",
    "             and imging data\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            isinstance(study_pair, (tuple, list)) and len(study_pair) == 2\n",
    "        ), \"Given argument isn't a pair of datasets\"\n",
    "        assert isinstance(study_pair[0], np.ndarray) or not isinstance(\n",
    "            study_pair[1], np.ndarray\n",
    "        ), \"Data provided ins't numpy array\"\n",
    "        assert (study_pair[0].shape[0] != 0) and (\n",
    "            study_pair[1].shape[0] != 0\n",
    "        ), \"Study pairs contains not data\"\n",
    "        assert (\n",
    "            study_pair[0].shape[0] == study_pair[1].shape[0]\n",
    "        ), f\"Mismatch between ({study_pair[0].shape[0]} and {study_pair[1].shape[0]})\"\n",
    "\n",
    "    def _optimise(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to minimise the\n",
    "        objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        model = minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            options={\"gtol\": self.tol_, \"maxiter\": self.maxiter_},\n",
    "            args=(self.covariances_, self.theta_, self.l2_),\n",
    "        )\n",
    "        self.best_loss = model.fun\n",
    "        self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "    def _get_dimensions(self, *data_sets) -> None:\n",
    "        \"\"\"\n",
    "        Method to get the dimensions\n",
    "        of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *data_sets: tuple\n",
    "            tuple of datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dims_ = [(X.shape[1], Y.shape[1]) for X, Y in data_sets]\n",
    "\n",
    "    def _split_weights(self, weights: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector weights into individual vectors\n",
    "        for each x and b dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            flatten numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        split_weights: list[np.ndarray]\n",
    "            list of weights split\n",
    "            wx and wb\n",
    "\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        split_weights = []\n",
    "        for X_dim, Y_dim in self.dims_:\n",
    "            wx = weights[offset : offset + X_dim]\n",
    "            offset += X_dim\n",
    "            wb = weights[offset : offset + Y_dim]\n",
    "            offset += Y_dim\n",
    "            split_weights.append((wx, wb))\n",
    "        return split_weights\n",
    "\n",
    "    def _objective_function(\n",
    "        self, weights: np.ndarray, covariances: dict, theta: float, l2: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Objective function of the CA3 class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            weights\n",
    "        covariances: dict\n",
    "            dict of cross/auto covariance\n",
    "            matricies\n",
    "        theta: float\n",
    "            theta penality\n",
    "        l2: float\n",
    "            regularization penailty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        total_loss: float\n",
    "           total loss of the objective function\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        weights_ = self._split_weights(weights)\n",
    "        for idx, (wx, wb) in enumerate(weights_):\n",
    "            s_xb = covariances[f\"s_X{idx+1}_Y{idx+1}\"]\n",
    "            s_xx = covariances[f\"s_X{idx+1}_X{idx+1}\"]\n",
    "            s_bb = covariances[f\"s_Y{idx+1}_Y{idx+1}\"]\n",
    "            total_loss += self._cross_cov_term(wb, s_xb, wx)\n",
    "            total_loss += self._regularization_term(wx, s_xx, l2)\n",
    "            total_loss += self._regularization_term(wb, s_bb, l2)\n",
    "            \n",
    "        # Similarity penalty across imaging weights\n",
    "        if theta > 0 and len(weights_) > 1:\n",
    "            total_loss += sum(\n",
    "                self._dissimilarity_penality(theta, w1[0], w2[0])\n",
    "                for w1, w2 in combinations(weights_, 2)\n",
    "            )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def _create_covariance_matrix(\n",
    "        self, matrix_1: np.ndarray, matrix_2: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cross-auto\n",
    "        covariance matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should\n",
    "            correspond to subject by\n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should\n",
    "            correspond to features by\n",
    "            feautres\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0]\n",
    "\n",
    "    def _normalise(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to normalise data.\n",
    "\n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        dmean = data - data.mean(axis=0)\n",
    "        std = data.std(axis=0, ddof=1)\n",
    "        std = np.where(std == 0.0, 1.0, std)\n",
    "        return dmean / std\n",
    "\n",
    "    def _cross_cov_term(\n",
    "        self, weight_Y: np.ndarray, cov_mat: np.ndarray, weight_X: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to calculate the cross covarance term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_Y: np.ndarray\n",
    "            set of weights for wb\n",
    "        cov_mat: np.ndarray\n",
    "             covariance matrix for\n",
    "             wx wb\n",
    "        weight_X: np.ndarray\n",
    "            set of weights for wx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: np.array\n",
    "            cross covariance term\n",
    "        \"\"\"\n",
    "        return -weight_X.T @ (cov_mat @ weight_Y)\n",
    "\n",
    "    def _regularization_term(\n",
    "        self, weight: np.ndarray, cov_mat: np.ndarray, lambda_i: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the regularization term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight: np.ndarray\n",
    "            set of weights\n",
    "        cov_mat: np.ndarray\n",
    "            auto covariance matrix\n",
    "        lambda_i: float\n",
    "            regularization parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            regularization term of the objective function\n",
    "        \"\"\"\n",
    "        return 0.5 * lambda_i * (weight.T @ (cov_mat @ weight) -1)\n",
    "\n",
    "    def _dissimilarity_penality(\n",
    "        self, theta_r: float, X_weight1: np.ndarray, X_weight2: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to return dissimilarity penality\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        theta_r: float\n",
    "           theta penality.\n",
    "        img_weight1: np.ndarray\n",
    "            weights of imaging data\n",
    "        img_weight2: np.ndarray\n",
    "            weights of second imaging\n",
    "            data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            dissimilarity penality\n",
    "        \"\"\"\n",
    "        return theta_r * 0.5 * np.sum((X_weight1 - X_weight2) ** 2)\n",
    "\n",
    "    def _score(self, *data_sets: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Method used to evaluate model performance.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            mean of correlation\n",
    "            values across datasets\n",
    "\n",
    "        \"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before scoring.\")\n",
    "\n",
    "        self.transform(*data_sets)\n",
    "        correlations = self.calculate_canonical_correlations()\n",
    "        return np.mean(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5254108",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca3 = C3A(l2=0.0)\n",
    "ca3.fit(study1)\n",
    "transfomed = ca3.fit_transform(study1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9284e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.3239853848670129)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.calculate_canonical_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b23471a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 6.50947842e-02,  3.45557537e-01,  5.66321994e-01,  3.83312821e-01,\n",
       "         -1.81847950e-01,  2.62204233e-01, -5.05192906e-02, -4.02943852e-02,\n",
       "         -2.99698561e-02,  4.20505671e-02,  1.26363061e-01,  1.42736771e-01,\n",
       "         -8.92349301e-02,  2.41565788e-02,  9.41206529e-02,  8.35883746e-02,\n",
       "          6.84212524e-02,  5.83734116e-02, -5.86919099e-02, -1.82156739e-01,\n",
       "          9.92557349e-02,  1.34787598e-01, -5.19479026e-04,  1.16005295e-01,\n",
       "         -1.37646563e-01,  7.99917716e-02, -7.39660771e-02,  1.47775193e-01,\n",
       "          1.09178108e-01, -5.99257009e-02, -8.81859588e-02, -1.60744524e-01,\n",
       "         -9.58841613e-02, -4.96039626e-02, -3.10735979e-02,  8.29625651e-02,\n",
       "          1.75327875e-01, -1.31592730e-03, -1.12143690e-02, -1.27730866e-01,\n",
       "         -5.07038437e-02, -2.00497007e-01,  1.02188325e-01,  8.78384925e-03,\n",
       "         -2.13156732e-02, -1.12533137e-01,  3.32505881e-02, -5.55617049e-02,\n",
       "         -1.50266855e-02,  1.06955485e-01]),\n",
       "  array([1.]))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.compute_loadings(study1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3adf3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2: 0.01, score: 0.32219472953669887\n",
      "l2: 0.1, score: 0.32219472953669887\n",
      "l2: 1.0, score: -0.32219472953669887\n",
      "l2: 10.0, score: 0.3221947295366989\n",
      "l2: 1000000, score: 0.32219472953669887\n",
      "Best l2: 10.0, Best score: 0.3221947295366989\n",
      "Best L2: 10.0\n",
      "Best score: 0.3221947295366989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming `data` is a list of tuples: [(img1, beh1), (img2, beh2), ...]\n",
    "# Define a custom scorer if needed, but the `score` method should suffice\n",
    "\n",
    "search = GridSearchCA3(l2_values=[0.01, 0.1, 1.0, 10.0, 1000000], theta=0.5, verbose=True)\n",
    "search.fit(study1, study2)\n",
    "\n",
    "best_model = search.get_best_model()\n",
    "print(\"Best L2:\", search.get_best_l2())\n",
    "print(\"Best score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c618128",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x_loadings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x, x_p) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_p, x_p)\n\u001b[1;32m      6\u001b[0m y_loadings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y, y_p) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y_p, y_p)\n\u001b[0;32m----> 7\u001b[0m x_rotations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x, \u001b[43mpinv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_loadings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_loadings)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_loadings)\n",
      "File \u001b[0;32m~/envs/cca/lib/python3.10/site-packages/scipy/linalg/_basic.py:1607\u001b[0m, in \u001b[0;36mpinv\u001b[0;34m(a, atol, rtol, return_rank, check_finite)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03mCompute the (Moore-Penrose) pseudo-inverse of a matrix.\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m a \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[0;32m-> 1607\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43m_decomp_svd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m t \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m   1609\u001b[0m maxS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(s, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/envs/cca/lib/python3.10/site-packages/scipy/linalg/_decomp_svd.py:108\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    106\u001b[0m a1 \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m m, n \u001b[38;5;241m=\u001b[39m a1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# accommodate empty matrix\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected matrix"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import pinv as pinv2\n",
    "for (x, y), (x_p, y_p) in zip(ca3.data, ca3.projects_):\n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    x_loadings = np.dot(x, x_p) / np.dot(x_p, x_p)\n",
    "    y_loadings = np.dot(y, y_p) / np.dot(y_p, y_p)\n",
    "    x_rotations = np.dot(x, pinv2(np.dot(x_loadings.T, x )))\n",
    "    print(x_loadings)\n",
    "    print(y_loadings)\n",
    "    #print(y.shape[0])\n",
    "    #print(x_p.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a05cce5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m X1_proj_ca3, Y1_proj_ca3 \u001b[38;5;241m=\u001b[39m \u001b[43mtransfomed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprojections\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudy0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m display(scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mttest_ind(X1_proj_ca3\u001b[38;5;241m.\u001b[39mflatten(), X1_proj_cca\u001b[38;5;241m.\u001b[39mflatten() ))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats \n",
    "X1_proj_ca3, Y1_proj_ca3 = transfomed['projections']['study0']\n",
    "display(scipy.stats.ttest_ind(X1_proj_ca3.flatten(), X1_proj_cca.flatten() ))\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X1_proj_ca3, Y1_proj_ca3, c='teal')\n",
    "m, b = np.polyfit(X1_proj_ca3.flatten(), Y1_proj_ca3.flatten(), 1)\n",
    "plt.plot(X1_proj_ca3, m*X1_proj_ca3 + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {transfomed['correlations']['study0'][0]:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"CA3 projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X1_proj_cca, Y1_proj_cca, c='orange', label=f\"r = {sklearn_corr:.2f}\")\n",
    "m, b = np.polyfit(X1_proj_cca.flatten(), Y1_proj_cca.flatten(), 1)\n",
    "plt.plot(X1_proj_cca, m*X1_proj_cca + b, color='black', linestyle='--')\n",
    "plt.text(0.05, 0.95, f\"r = {sklearn_corr:.2f}\", \n",
    "         transform=plt.gca().transAxes, va='top', ha='left')\n",
    "plt.title(\"sklearn CCA projections\")\n",
    "plt.xlabel(\"Imaging\")\n",
    "plt.ylabel(\"Behavior\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class C3A_single_study:\n",
    "    \"\"\"\n",
    "    C3A class.\n",
    "    A class to do C3A\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    c3a = C3A(l2=0.5, theta=1)\n",
    "    c3a.fit(study1, study2)\n",
    "    transformed = c3a.transform(study1, study2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2: float = 1,\n",
    "        theta: float = 0,\n",
    "        tol=1e-6,\n",
    "        maxiter=500,\n",
    "        normalise_weights=True,\n",
    "    ):\n",
    "        self.l2_ = l2\n",
    "        self.theta_ = theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        self.tol_ = tol\n",
    "        self.maxiter_ = maxiter\n",
    "        self.normalise_weights = normalise_weights\n",
    "        self.canonical_correlations_ = None\n",
    "        self.projections_ = None\n",
    "\n",
    "    def fit(self, X, Y) -> None:\n",
    "        \"\"\"\n",
    "        Method to fit the CA3 model to a given\n",
    "        set of datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._calculate_covariance_matricies(X, Y)\n",
    "        self._get_dimensions(X, Y)\n",
    "        self._weight_intialization()\n",
    "        self._optimise()\n",
    "\n",
    "    def transform(self, X, Y: tuple) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to transform data sets into canonical\n",
    "        projects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.weights_ is not None\n",
    "        ), \"Model must be fitted before transform can be called.\"\n",
    "\n",
    "        x_projections = self._normalise(self._normalise(X) @ self.weights_[0])\n",
    "        y_projections = self._normalise(self._normalise(Y) @ self.weights_[1])\n",
    "        self.projections_ = np.stack([x_projections, y_projections])\n",
    "        self.canonical_correlations_ = np.corrcoef(x_projections, y_projections)[0, 1] \n",
    "        return self.projections_\n",
    "\n",
    "    def fit_transform(self, X, Y) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to fit a CA3 model and then transform\n",
    "        the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples.\n",
    "        \"\"\"\n",
    "        self.fit(X, Y)\n",
    "        return self.transform(X, Y)\n",
    "\n",
    "    def calculate_canonical_correlations(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Method to obtain the canonical correlations.\n",
    "        Model must have been fitted and transfomed\n",
    "        before.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        canonical_correlations: list[float]\n",
    "            list of canonical correlations\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.canonical_correlations_ is not None\n",
    "        ), \"Model must be fitted and transfomed before correlations can be returned\"\n",
    "        return self.canonical_correlations_\n",
    "        \n",
    "    def compute_loadings(self, X, Y) -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Computes canonical loadings for each study.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (img_data, beh_data) pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loadings: list of tuples\n",
    "            Each tuple contains (img_loadings, beh_loadings), i.e., correlations between\n",
    "            original features and their respective canonical variates.\n",
    "        \"\"\"\n",
    "        assert self.projections_ is not None, \"Model must be fitted and transfomed before computing loadings.\"\n",
    "        return [\n",
    "            np.corrcoef(self._normalise(X).T, self.projections_[0], rowvar=True)[:-1, -1],\n",
    "            np.corrcoef(self._normalise(Y).T, self.projections_[1], rowvar=True)[:-1, -1]\n",
    "            ]\n",
    "    \n",
    "    def _weight_intialization(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to define a set of random starting\n",
    "        weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\"\n",
    "\n",
    "        s_xb = self.covariances_[f\"s_X_Y\"]\n",
    "        try:\n",
    "            U, _, Vt = np.linalg.svd(s_xb, full_matrices=False)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            raise RuntimeError(f\"SVD failed due to: {e}\")\n",
    "\n",
    "        wx = U[:, 0]\n",
    "        wb = Vt.T[:, 0]\n",
    "        s_xx = self.covariances_[f\"s_X_X\"]\n",
    "        s_bb = self.covariances_[f\"s_Y_Y\"]\n",
    "\n",
    "        wx = wx / np.sqrt(wx.T @ s_xx @ wx)\n",
    "        wb = wb / np.sqrt(wb.T @ s_bb @ wb)\n",
    "        self.intial_weights_ = np.concat([wx, wb])\n",
    "\n",
    "    def _calculate_covariance_matricies(self, X_data, Y_data) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance and auto covariance\n",
    "        matricies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "        \"\"\"\n",
    "        self._data_able_to_process(X_data, Y_data)\n",
    "        X_data = self._normalise(X_data)\n",
    "        Y_data = self._normalise(Y_data)\n",
    "\n",
    "        try:\n",
    "            self.covariances_[\"s_Y_Y\"] = (\n",
    "                self._create_covariance_matrix(Y_data, Y_data)\n",
    "            )\n",
    "            self.covariances_[f\"s_X_X\"] = (\n",
    "                self._create_covariance_matrix(X_data, X_data)\n",
    "            )\n",
    "            self.covariances_[f\"s_X_Y\"] = (\n",
    "                self._create_covariance_matrix(X_data, Y_data)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances due to: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, X_data, Y_data) -> bool:\n",
    "        \"\"\"\n",
    "        Method to check that data\n",
    "        is in correct format to be processed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple,\n",
    "             tuple of behavioural data\n",
    "             and imging data\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        assert isinstance(Y_data, np.ndarray) or not isinstance(\n",
    "            X_data, np.ndarray\n",
    "        ), \"Data provided ins't numpy array\"\n",
    "        assert (X_data.shape[0] != 0) and (\n",
    "            Y_data.shape[0] != 0\n",
    "        ), \"Study pairs contains not data\"\n",
    "        assert (\n",
    "            X_data.shape[0] == Y_data.shape[0]\n",
    "        ), f\"Mismatch between ({X_data.shape[0]} and {Y_data.shape[0]})\"\n",
    "\n",
    "    def _optimise(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to minimise the\n",
    "        objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        model = minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            options={\"gtol\": self.tol_, \"maxiter\": self.maxiter_},\n",
    "            args=(self.covariances_, self.theta_, self.l2_),\n",
    "        )\n",
    "        self.best_loss = model.fun\n",
    "        self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "    def _get_dimensions(self, X, Y) -> None:\n",
    "        \"\"\"\n",
    "        Method to get the dimensions\n",
    "        of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *data_sets: tuple\n",
    "            tuple of datasets\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dims_ = [X.shape[1], Y.shape[1]]\n",
    "\n",
    "    def _split_weights(self, weights: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector weights into individual vectors\n",
    "        for each x and b dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            flatten numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        split_weights: list[np.ndarray]\n",
    "            list of weights split\n",
    "            wx and wb\n",
    "\n",
    "        \"\"\"\n",
    "        wx = weights[0 : self.dims_[0]]\n",
    "        wb = weights[self.dims_[0]:  self.dims_[1]+1]\n",
    "        return [wx, wb]\n",
    "\n",
    "    def _objective_function(\n",
    "        self, weights: np.ndarray, covariances: dict, theta: float, l2: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Objective function of the CA3 class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            weights\n",
    "        covariances: dict\n",
    "            dict of cross/auto covariance\n",
    "            matricies\n",
    "        theta: float\n",
    "            theta penality\n",
    "        l2: float\n",
    "            regularization penailty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        total_loss: float\n",
    "           total loss of the objective function\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        wx, wb = self._split_weights(weights)\n",
    "        s_xb = covariances[\"s_X_Y\"]\n",
    "        s_xx = covariances[\"s_X_X\"]\n",
    "        s_bb = covariances[\"s_Y_Y\"]\n",
    "        total_loss += self._cross_cov_term(wb, s_xb, wx)\n",
    "        total_loss += self._regularization_term(wx, s_xx, l2)\n",
    "        total_loss += self._regularization_term(wb, s_bb, l2)\n",
    "\n",
    "        ## Similarity penalty across imaging weights\n",
    "        #if theta > 0 and len(weights_) > 1:\n",
    "        #    total_loss += sum(\n",
    "        #        self._dissimilarity_penality(theta, w1[0], w2[0])\n",
    "        #        for w1, w2 in combinations(weights_, 2)\n",
    "        #    )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def _create_covariance_matrix(\n",
    "        self, matrix_1: np.ndarray, matrix_2: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cross-auto\n",
    "        covariance matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should\n",
    "            correspond to subject by\n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should\n",
    "            correspond to features by\n",
    "            feautres\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0]\n",
    "\n",
    "    def _normalise(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to normalise data.\n",
    "\n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        dmean = data - data.mean(axis=0)\n",
    "        std = data.std(axis=0, ddof=1)\n",
    "        std = np.where(std == 0.0, 1.0, std)\n",
    "        return dmean / std\n",
    "\n",
    "    def _cross_cov_term(\n",
    "        self, weight_Y: np.ndarray, cov_mat: np.ndarray, weight_X: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to calculate the cross covarance term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_Y: np.ndarray\n",
    "            set of weights for wb\n",
    "        cov_mat: np.ndarray\n",
    "             covariance matrix for\n",
    "             wx wb\n",
    "        weight_X: np.ndarray\n",
    "            set of weights for wx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: np.array\n",
    "            cross covariance term\n",
    "        \"\"\"\n",
    "        return -weight_X.T @ (cov_mat @ weight_Y)\n",
    "\n",
    "    def _regularization_term(\n",
    "        self, weight: np.ndarray, cov_mat: np.ndarray, lambda_i: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the regularization term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight: np.ndarray\n",
    "            set of weights\n",
    "        cov_mat: np.ndarray\n",
    "            auto covariance matrix\n",
    "        lambda_i: float\n",
    "            regularization parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            regularization term of the objective function\n",
    "        \"\"\"\n",
    "        return 0.5 * lambda_i * (weight.T @ (cov_mat @ weight) - 1)\n",
    "\n",
    "    def _dissimilarity_penality(\n",
    "        self, theta_r: float, X_weight1: np.ndarray, X_weight2: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to return dissimilarity penality\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        theta_r: float\n",
    "           theta penality.\n",
    "        img_weight1: np.ndarray\n",
    "            weights of imaging data\n",
    "        img_weight2: np.ndarray\n",
    "            weights of second imaging\n",
    "            data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            dissimilarity penality\n",
    "        \"\"\"\n",
    "        return theta_r * 0.5 * np.sum((X_weight1 - X_weight2) ** 2)\n",
    "\n",
    "    def _score(self, X, Y) -> float:\n",
    "        \"\"\"\n",
    "        Method used to evaluate model performance.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            mean of correlation\n",
    "            values across datasets\n",
    "\n",
    "        \"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before scoring.\")\n",
    "\n",
    "        self.transform(X, Y)\n",
    "        correlations = self.calculate_canonical_correlations()\n",
    "        return np.mean(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d957c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
