{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d98ad7c",
   "metadata": {},
   "source": [
    "# CA3 model\n",
    "\n",
    "This notebook is for building out the CA3 model as previous implementation doesn't work.\n",
    "\n",
    "Formula is:\n",
    "$$\n",
    "min_{\\vec{w}_{x_1}, \\vec{w}_{x_2}, \\vec{w}_a, \\vec{w}_b} \\left( -\\vec{w}_{x_1}^T S_{xa} \\vec{w}_a - \\vec{w}_{x_2}^T S_{xb} \\vec{w}_b + \\sum_{i \\in \\{x_1, x_2, a, b\\}} \\frac{1}{2} \\lambda_i \\left( \\vec{w}_i^T S_{ii} \\vec{w}_i - 1 \\right) + \\theta_{rr}(\\vec{w}_{x_1}, \\vec{w}_{x_2}) \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0861b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemmr.generative_model import GEMMR\n",
    "from gemmr.estimators import SVDCCA\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "import scipy.stats\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d9b9d",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38d98",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 200\n",
    "n_features_x = 10\n",
    "n_features_y = 8\n",
    "\n",
    "# Latent variables (shared source of variance)\n",
    "latent = np.random.randn(n_samples, 3)\n",
    "\n",
    "# Mix latent variables with some noise\n",
    "behavioural_data_study1 = latent @ np.random.randn(3, n_features_x) + 0.1 * np.random.randn(n_samples, n_features_x)\n",
    "imging_data_study1 = latent @ np.random.randn(3, n_features_y) + 0.1 * np.random.randn(n_samples, n_features_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf757017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition = GEMMR('cca', wx=2, wy=50, r_between=0.3)\n",
    "behavioural_data_study1, imging_data_study1 = model_definition.generate_data(n=200)\n",
    "behavioural_data_study2, imging_data_study2 = model_definition.generate_data(n=190)\n",
    "study1 = (imging_data_study1, behavioural_data_study1) \n",
    "study2 = (imging_data_study2, behavioural_data_study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dd3d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.004693325562976217)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2= []\n",
    "for val in range(imging_data_study1.shape[1]):\n",
    "    corr = scipy.stats.pearsonr(behavioural_data_study1[:, 1], imging_data_study1[:, val])[0]\n",
    "    r2.append(corr)\n",
    "np.mean(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc9559",
   "metadata": {},
   "source": [
    "## Step 1: Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to demean data.\n",
    "\n",
    "    Parmeteres\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data to demean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        demeaned data\n",
    "    \"\"\"\n",
    "    return data - data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0a8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov(matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to calculate \n",
    "    covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_1: np.ndarray\n",
    "        A matrix tht should \n",
    "        correspond to subject by \n",
    "        features\n",
    "    matrix_2: np.ndarray\n",
    "        A matrix that should \n",
    "        correspond to features by\n",
    "        feautres \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray: array\n",
    "        array of cross covariance matrix\n",
    "    \"\"\"\n",
    "    return (matrix_1.T @ matrix_2) / matrix_1.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54813454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_able_to_process(study_pair: tuple, behav_data: np.ndarray, img_data: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check that data\n",
    "    is in correct format to be processed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "     study_pair: tuple, \n",
    "         tuple of behavioural data \n",
    "         and imging data\n",
    "     behav_data: np.ndarray\n",
    "         array of behav_data \n",
    "     img_data: np.ndarray\n",
    "         array of img_data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool: boolean\n",
    "        bool of if failed or not\n",
    "    \"\"\"\n",
    "    if not isinstance(study_pair, (tuple, list)) or len(study_pair) != 2:\n",
    "        print(\"Given argument isn't a pair of datasets\")\n",
    "        return False\n",
    "    if not isinstance(behav_data, np.ndarray) or not isinstance(img_data, np.ndarray):\n",
    "        print(\"Data provided isn't a numpy array\")\n",
    "        return False\n",
    "    if behav_data.shape[0] == 0 or img_data.shape[0] == 0 or behav_data.shape[0] != img_data.shape[0]:\n",
    "        print(f\"Mismatch between ({behav_data.shape[0]} and {img_data.shape[0]})\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dbb76963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_matricies(*study_pairs) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates covariance matrices and auto covariance\n",
    "    matricies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_pairs: tuple\n",
    "        a tuple or list containing two numpy arrays:\n",
    "        (behavioural_data, imaging_data).\n",
    "        Assumes data is (subjects x features).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    covariance_results: dict\n",
    "        dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "    \"\"\"\n",
    "    covariance_results = {}\n",
    "    for idx, study_pair in enumerate(study_pairs):\n",
    "        img_data, behav_data  = study_pair\n",
    "        if not data_able_to_process(study_pair, behav_data, img_data):\n",
    "            continue\n",
    "        behav_data = mean_center(behav_data)\n",
    "        img_data = mean_center(img_data)\n",
    "        study_num = idx + 1\n",
    "        try:\n",
    "            covariance_results[f\"s_behav{study_num}_behav{study_num}\"] = cross_cov(behav_data, behav_data)\n",
    "            covariance_results[f\"s_img{study_num}_img{study_num}\"] = cross_cov(img_data, img_data)\n",
    "            covariance_results[f\"s_img{study_num}_behav{study_num}\"] = cross_cov(img_data, behav_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "            return None\n",
    "    return covariance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd37f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_mat = calculate_covariance_matricies(study1, study2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "36418b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x1a = cross_cov(behavioural_data_study1, imging_data_study1)\n",
    "s_x2b = cross_cov(behavioural_data_study2, imging_data_study2)\n",
    "s_x1x1 = cross_cov(behavioural_data_study1, behavioural_data_study1)\n",
    "s_x2x2 = cross_cov(behavioural_data_study2, behavioural_data_study2)\n",
    "s_aa = cross_cov(imging_data_study1, imging_data_study1)\n",
    "s_bb = cross_cov(imging_data_study2, imging_data_study2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040469c",
   "metadata": {},
   "source": [
    "## Step 2. Intialization of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9220d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_intialization(*weights) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Define a set of random starting \n",
    "    weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: tuple(int)\n",
    "        tuple of set amount\n",
    "        of int values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarrray\n",
    "        array of numpy values\n",
    "    \"\"\"\n",
    "    return np.random.randn(sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d329c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = {\n",
    "  '1': behavioural_data_study1.shape[1], \n",
    "   '2': behavioural_data_study2.shape[1],\n",
    "    '3': imging_data_study1.shape[1],\n",
    "    '4': imging_data_study2.shape[1]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6c66dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0 = weight_intialization(\n",
    "    behavioural_data_study1.shape[1], \n",
    "    behavioural_data_study2.shape[1],\n",
    "    imging_data_study1.shape[1],\n",
    "    imging_data_study2.shape[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b44fd",
   "metadata": {},
   "source": [
    "## Step 3. Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db5333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx1_shape = s_x1x1.shape[0]\n",
    "dx2_shape = s_x2x2.shape[0]\n",
    "da_shape = s_aa.shape[0]\n",
    "db_shape = s_bb.shape[0]\n",
    "dx_shape = dx1_shape + dx2_shape\n",
    "dac_shape =  dx_shape + da_shape\n",
    "\n",
    "def get_dimensions(s_x1x1, s_x2x2, s_aa):\n",
    "    dx1_shape = s_x1x1.shape[0]\n",
    "    dx2_shape = s_x2x2.shape[0]\n",
    "    da_shape = s_aa.shape[0]\n",
    "    dx_shape =  dx1_shape + dx2_shape\n",
    "    return {\n",
    "        'dx1_shape': dx1_shape,\n",
    "        'dx_shape' : dx_shape,\n",
    "        'dac_shape':  dx_shape + da_shape\n",
    "    }\n",
    "\n",
    "def get_weights(weight_array, dx1_shape, dx_shape, dac_shape):\n",
    "    return {\n",
    "        \"wx1\": weight_array[:dx1_shape],\n",
    "        \"wx2\": weight_array[dx1_shape:dx_shape],\n",
    "        \"wa\": weight_array[dx_shape:dac_shape],\n",
    "        \"wb\": weight_array[dac_shape:]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43838fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov_term(weight_beh, cov_mat, weight_img):\n",
    "    return -weight_img.T @ (cov_mat @ weight_beh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a0332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_term(weight, cov_mat):\n",
    "    return 0.5 * 1.0 * (weight.T @ (cov_mat @ weight) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity_penality(theta_r, img_weight1, img_weight2):\n",
    "    return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c79bec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(weights, s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r):\n",
    "    dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "    weights = get_weights(\n",
    "            weights, \n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])\n",
    "    term1 = cross_cov_term(weights['wx1'],s_x1a, weights['wa'])\n",
    "    term2 = cross_cov_term(weights['wx2'],s_x2b, weights['wb'])\n",
    "    reg_x1 = regularization_term(weights['wx1'], s_x1x1)\n",
    "    reg_x2 = regularization_term(weights['wx2'], s_x2x2)\n",
    "    reg_a = regularization_term(weights['wa'], s_aa)\n",
    "    reg_b = regularization_term(weights['wb'], s_bb)\n",
    "    theta_r = dissimilarity_penality(theta_r, weights['wa'], weights['wb'])\n",
    "    return term1 + term2 + reg_x1 + reg_x2 + reg_a + reg_b + theta_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9f1c9",
   "metadata": {},
   "source": [
    "## Step 4: Minimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee3de1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "Best θ_r: 0.046415888336127795\n",
      "Best loss: -1.9999999921047418\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "optimal_theta_r = None\n",
    "optimium_model = None\n",
    "\n",
    "for theta_r in np.logspace(-3, 2, 10):\n",
    "    weights_0 = weight_intialization(\n",
    "        behavioural_data_study1.shape[1], \n",
    "        behavioural_data_study2.shape[1],\n",
    "        imging_data_study1.shape[1],\n",
    "        imging_data_study2.shape[1])  # re-init each time\n",
    "    res = minimize(\n",
    "        objective_function,\n",
    "        weights_0,\n",
    "        args=(s_x1a, s_x2b, s_x1x1, s_x2x2, s_aa, s_bb, theta_r),\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    if res.status !=0:\n",
    "        print(res.status)\n",
    "        continue\n",
    "\n",
    "\n",
    "    if res.fun < best_loss:\n",
    "        best_loss = res.fun\n",
    "        best_theta_r = theta_r\n",
    "        optimium_model = res\n",
    "\n",
    "print(f\"Best θ_r: {best_theta_r}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5635b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = get_dimensions(s_x1x1, s_x2x2, s_aa)\n",
    "weights = get_weights(optimium_model.x,\n",
    "            dimensions['dx1_shape'], \n",
    "            dimensions['dx_shape'], \n",
    "            dimensions['dac_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0d427de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get projections (scores)\n",
    "scores_x1 = behavioural_data_study1 @ weights['wx1']\n",
    "scores_x2 = behavioural_data_study2 @ weights['wx2']\n",
    "scores_a  = imging_data_study1 @ weights['wa']\n",
    "scores_b  = imging_data_study2 @ weights['wb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac5b74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.11538425261898907)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.07889484464506155)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.00014508777456870848)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(np.corrcoef(scores_x1, scores_a)[0, 1])\n",
    "display(np.corrcoef(scores_x2, scores_b)[0, 1])\n",
    "display(np.linalg.norm(weights['wa'] - weights['wb']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfca62",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c072048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CA3:\n",
    "    def __init__(self, theta: float=None, random_seed: int=None):\n",
    "        self.theta_ = np.logspace(-3, 2, 10) if theta is None else theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float('inf')\n",
    "        self.optimal_theta_r = None\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        random_seed = 42 if random_seed is None else random_seed\n",
    "        self.rng = np.random.RandomState(random_seed) \n",
    "\n",
    "    def fit(self, *data_sets):\n",
    "        self._calculate_covariance_matricies(*data_sets)\n",
    "        self._get_dimensions(*data_sets)\n",
    "        self._weight_intialization(*list(itertools.chain(*self.dims_)))\n",
    "        self._optimise()\n",
    "\n",
    "    def transform(self, *data_sets):\n",
    "        assert self.weights_ is not None, \"Model must be fitted before transfomed can be called.\"\n",
    "        assert len(data_sets) == len(self.dims_), \"Model fitted with different number of datasets.\"\n",
    "        \n",
    "        scores = {}\n",
    "        correlations = {}\n",
    "        count = 0\n",
    "        for (img_data, beh_data), (wx, wb) in zip(data_sets, self.weights_):\n",
    "            imging_projections = img_data @ wx\n",
    "            beh_projections = beh_data @ wb\n",
    "            scores[f'study{count}'] = [imging_projections, beh_projections]\n",
    "            corr = np.array([np.corrcoef(imging_projections, beh_projections)[0, 1]])\n",
    "            correlations[f'study{count}'] = corr\n",
    "            count += 1\n",
    "    \n",
    "        return {\n",
    "              \"correlations\": correlations, \n",
    "              \"projections\": scores\n",
    "        }\n",
    "    \n",
    "    def fit_transform(self, *data_sets):\n",
    "        self.fit(*data_sets)\n",
    "        return self.transform(*data_sets)\n",
    "    \n",
    "    def _weight_intialization(self, *weights) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Define a set of random starting \n",
    "        weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\" \n",
    "        self.intial_weights_ = self.rng.randn(sum(weights))  # Assigning the result to the instance attribute\n",
    "\n",
    "    def _calculate_covariance_matricies(self, *data_sets) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance matrices and auto covariance\n",
    "        matricies\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "    \n",
    "        \"\"\"\n",
    "        for idx, study_pair in enumerate(data_sets):\n",
    "            img_data, behav_data  = study_pair\n",
    "            if not self._data_able_to_process(study_pair, behav_data, img_data):\n",
    "                continue\n",
    "            behav_data = self._mean_center(behav_data)\n",
    "            img_data = self._mean_center(img_data)\n",
    "            study_num = idx + 1\n",
    "            try:\n",
    "                self.covariances_[f\"s_behav{study_num}_behav{study_num}\"] = self._create_covariance_amtrix(behav_data, behav_data)\n",
    "                self.covariances_[f\"s_img{study_num}_img{study_num}\"] = self._create_covariance_amtrix(img_data, img_data)\n",
    "                self.covariances_[f\"s_img{study_num}_behav{study_num}\"] = self._create_covariance_amtrix(img_data, behav_data)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating covariances for Study {study_num}: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, study_pair: tuple, behav_data: np.ndarray, img_data: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Function to check that data\n",
    "        is in correct format to be processed\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple, \n",
    "             tuple of behavioural data \n",
    "             and imging data\n",
    "         behav_data: np.ndarray\n",
    "             array of behav_data \n",
    "         img_data: np.ndarray\n",
    "             array of img_data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        if not isinstance(study_pair, (tuple, list)) or len(study_pair) != 2:\n",
    "            print(\"Given argument isn't a pair of datasets\")\n",
    "            return False\n",
    "        if not isinstance(behav_data, np.ndarray) or not isinstance(img_data, np.ndarray):\n",
    "            print(\"Data provided isn't a numpy array\")\n",
    "            return False\n",
    "        if behav_data.shape[0] == 0 or img_data.shape[0] == 0 or behav_data.shape[0] != img_data.shape[0]:\n",
    "            print(f\"Mismatch between ({behav_data.shape[0]} and {img_data.shape[0]})\")\n",
    "    \n",
    "        return True\n",
    "            \n",
    "    def _optimise(self):\n",
    "        if isinstance(self.theta_, (list, np.ndarray)):\n",
    "            for theta in self.theta_:\n",
    "                model = self._optimising_model(theta)\n",
    "                if model.status !=0:\n",
    "                    continue\n",
    "                if model.fun < self.best_loss:\n",
    "                    self.best_loss = model.fun\n",
    "                    self.optimal_theta_r = theta\n",
    "                    self.weights_ = self._split_weights(model.x)\n",
    "        else:\n",
    "            model = self._optimising_model(theta)\n",
    "            self.best_loss = model.fun\n",
    "            self.optimal_theta_r = theta\n",
    "            self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "\n",
    "    def _optimising_model(self, theta):\n",
    "        return minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            args=(self.covariances_, theta),\n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "    \n",
    "    def _get_dimensions(self, *data_sets):\n",
    "        self.dims_ = [(behav.shape[1], img.shape[1]) for behav, img in data_sets]\n",
    "    \n",
    "    def _split_weights(self, w):\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector w into individual vectors\n",
    "        for each behavioural and imaging dataset.\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        weights = []\n",
    "        for img_dim, behav_dim in self.dims_:\n",
    "            wx = w[offset:offset + img_dim]\n",
    "            offset +=img_dim  \n",
    "            wb = w[offset:offset + behav_dim]\n",
    "            offset += behav_dim\n",
    "            weights.append((wx, wb))\n",
    "        return weights \n",
    "    \n",
    "    def _objective_function(self, weights, covariances, theta):\n",
    "        total_loss = 0\n",
    "        weights_ = self._split_weights(weights)\n",
    "        for idx, (wx, wb) in enumerate(weights_):\n",
    "           \n",
    "            s_xb = covariances[f\"s_img{idx+1}_behav{idx+1}\"]\n",
    "            s_xx = covariances[f\"s_img{idx+1}_img{idx+1}\"]\n",
    "            s_bb = covariances[f\"s_behav{idx+1}_behav{idx+1}\"]\n",
    "            total_loss += self._cross_cov_term(wb, s_xb, wx) \n",
    "            total_loss += self._regularization_term(wx, s_xx)\n",
    "            total_loss += self._regularization_term(wb, s_bb)\n",
    "    \n",
    "        # Similarity penalty across imaging weights\n",
    "        if theta > 0 and len(weights_) > 1:\n",
    "            for img_data in range(len(weights_)):\n",
    "                for next_img_data in range(img_data + 1, len(weights_)):\n",
    "                    total_loss += dissimilarity_penality(theta, weights_[img_data][0], weights_[next_img_data][0])\n",
    "    \n",
    "        return total_loss\n",
    "    \n",
    "    def _create_covariance_amtrix(self, matrix_1: np.ndarray, matrix_2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate \n",
    "        covariance matrix\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should \n",
    "            correspond to subject by \n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should \n",
    "            correspond to features by\n",
    "            feautres \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of cross covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0] \n",
    "    \n",
    "    def _mean_center(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to demean data.\n",
    "    \n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        return data - data.mean(axis=0)\n",
    "\n",
    "    def _cross_cov_term(self, weight_beh, cov_mat, weight_img):\n",
    "        return -weight_img.T @ (cov_mat @ weight_beh)\n",
    "\n",
    "    def _regularization_term(self, weight, cov_mat):\n",
    "        return 0.5 * 1.0 * (weight.T @ (cov_mat @ weight) - 1)\n",
    "\n",
    "    def _dissimilarity_penality(self, theta_r, img_weight1, img_weight2):\n",
    "        return theta_r * 0.5 * np.sum((img_weight1 - img_weight2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5254108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 2), (50, 2)]\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975\n",
      "  0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733\n",
      " -0.90802408 -1.4123037   1.46564877 -0.2257763   0.0675282  -1.42474819\n",
      " -0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      " -0.60170661  1.85227818 -0.01349722 -1.05771093  0.82254491 -1.22084365\n",
      "  0.2088636  -1.95967012 -1.32818605  0.19686124  0.73846658  0.17136828\n",
      " -0.11564828 -0.3011037  -1.47852199 -0.71984421 -0.46063877  1.05712223\n",
      "  0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "  1.03099952  0.93128012 -0.83921752 -0.30921238  0.33126343  0.97554513\n",
      " -0.47917424 -0.18565898 -1.10633497 -1.19620662  0.81252582  1.35624003\n",
      " -0.07201012  1.0035329   0.36163603 -0.64511975  0.36139561  1.53803657\n",
      " -0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
      "  0.09176078 -1.98756891 -0.21967189  0.35711257  1.47789404 -0.51827022\n",
      " -0.8084936  -0.50175704  0.91540212  0.32875111 -0.5297602   0.51326743\n",
      "  0.09707755  0.96864499 -0.70205309 -0.32766215 -0.39210815 -1.46351495\n",
      "  0.29612028  0.26105527  0.00511346 -0.23458713 -1.41537074 -0.42064532\n",
      " -0.34271452 -0.80227727]\n"
     ]
    }
   ],
   "source": [
    "ca3 = CA3()\n",
    "ca3.fit(study1, study2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ceafe101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca3.intial_weights_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
