{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527cbe52",
   "metadata": {},
   "source": [
    "# C3A implemenation\n",
    "\n",
    "THe one in the grant application isn't what we want. So this note book is trying to do \n",
    "\n",
    "Formula is:\n",
    "$$\n",
    "\\min_{\\vec{w}_{x_1}, \\vec{w}_{x_2}, \\vec{w}_a} \\left(\n",
    "\\theta_{rr}(\\vec{w}_{x_1}, \\vec{w}_{x_2})\n",
    "- \\vec{w}_{x_1}^\\top S_{xa} \\vec{w}_a\n",
    "+ \\sum_{i \\in \\{x_1, x_2, a\\}} \\frac{1}{2} \\lambda_i \\left( \\vec{w}_i^\\top S_{ii} \\vec{w}_i - 1 \\right)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszdjh3/envs/c3a/lib/python3.13/site-packages/gemmr/sample_size/linear_model.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    }
   ],
   "source": [
    "from gemmr.generative_model import GEMMR\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from gemmr.estimators import SVDCCA\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbe46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3A:\n",
    "    \"\"\"\n",
    "    C3A class.\n",
    "    A class to do C3A\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    c3a = C3A(l2=0.5, theta=1)\n",
    "    c3a.fit(study1, study2)\n",
    "    transformed = c3a.transform(study1, study2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2: float = 1,\n",
    "        theta: float = 0,\n",
    "        tol=1e-6,\n",
    "        maxiter=500,\n",
    "        normalise_weights=True,\n",
    "    ):\n",
    "        self.l2_ = l2\n",
    "        self.theta_ = theta\n",
    "        self.intial_weights_ = None\n",
    "        self.dims_ = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.weights_ = None\n",
    "        self.covariances_ = {}\n",
    "        self.tol_ = tol\n",
    "        self.maxiter_ = maxiter\n",
    "        self.normalise_weights = normalise_weights\n",
    "        self.canonical_correlations_ = None\n",
    "        self.projections_ = None\n",
    "\n",
    "\n",
    "    def fit(self, X, Y) -> None:\n",
    "        \"\"\"\n",
    "        Method to fit the CA3 model to a given\n",
    "        set of datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        X, Y = self._normalise_input_data(X, Y)\n",
    "        self._calculate_covariance_matricies(X, Y)\n",
    "        self._get_dimensions(X, Y)\n",
    "        self._weight_intialization()\n",
    "        self._optimise()\n",
    "\n",
    "    def transform(self, X, Y: tuple) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to transform data sets into canonical\n",
    "        projects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.weights_ is not None\n",
    "        ), \"Model must be fitted before transform can be called.\"\n",
    "\n",
    "        x_projections = self._normalise(self._normalise(X) @ self.weights_[0])\n",
    "        y_projections = self._normalise(self._normalise(Y) @ self.weights_[1])\n",
    "        self.projections_ = np.stack([x_projections, y_projections])\n",
    "        self.canonical_correlations_ = np.corrcoef(x_projections, y_projections)[0, 1] \n",
    "        return self.projections_\n",
    "\n",
    "    def fit_transform(self, X, Y) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Methods to fit a CA3 model and then transform\n",
    "        the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        projects: list[np.ndarray]\n",
    "            conatins a list of the\n",
    "            projections of each dataset in\n",
    "            ndarry of n_components by n_samples.\n",
    "        \"\"\"\n",
    "        self.fit(X, Y)\n",
    "        return self.transform(X, Y)\n",
    "\n",
    "    def calculate_canonical_correlations(self) -> list[float]:\n",
    "        \"\"\"\n",
    "        Method to obtain the canonical correlations.\n",
    "        Model must have been fitted and transfomed\n",
    "        before.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        canonical_correlations: list[float]\n",
    "            list of canonical correlations\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.canonical_correlations_ is not None\n",
    "        ), \"Model must be fitted and transfomed before correlations can be returned\"\n",
    "        return self.canonical_correlations_\n",
    "        \n",
    "    def compute_loadings(self, X, Y) -> list[tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Computes canonical loadings for each study.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (img_data, beh_data) pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loadings: list of tuples\n",
    "            Each tuple contains (img_loadings, beh_loadings), i.e., correlations between\n",
    "            original features and their respective canonical variates.\n",
    "        \"\"\"\n",
    "        assert self.projections_ is not None, \"Model must be fitted and transfomed before computing loadings.\"\n",
    "        return [\n",
    "            np.corrcoef(self._normalise(X).T, self.projections_[0], rowvar=True)[:-1, -1],\n",
    "            np.corrcoef(self._normalise(Y).T, self.projections_[1], rowvar=True)[:-1, -1]\n",
    "            ]\n",
    "    def _normalise_input_data(self, X, Y) -> tuple:\n",
    "        \"\"\"\n",
    "        Normalise input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_sets: tuple\n",
    "            List of (X, Y) pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data_set: tuple\n",
    "            tuple of normalised data\n",
    "        \"\"\"\n",
    "        return self._normalise(X), self._normalise(Y)\n",
    "    \n",
    "    def _weight_intialization(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to define a set of random starting\n",
    "        weights\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: tuple(int)\n",
    "            tuple of set amount\n",
    "            of int values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarrray\n",
    "            array of numpy values\n",
    "        \"\"\"\n",
    "\n",
    "        s_xb = self.covariances_[f\"s_X_Y\"]\n",
    "        try:\n",
    "            U, _, Vt = np.linalg.svd(s_xb, full_matrices=False)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            raise RuntimeError(f\"SVD failed due to: {e}\")\n",
    "\n",
    "        wx = U[:, 0]\n",
    "        wb = Vt.T[:, 0]\n",
    "        s_xx = self.covariances_[f\"s_X_X\"]\n",
    "        s_bb = self.covariances_[f\"s_Y_Y\"]\n",
    "\n",
    "        wx = wx / np.sqrt(wx.T @ s_xx @ wx)\n",
    "        wb = wb / np.sqrt(wb.T @ s_bb @ wb)\n",
    "        self.intial_weights_ = np.concat([wx, wb])\n",
    "\n",
    "    def _calculate_covariance_matricies(self, X_data, Y_data) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates covariance and auto covariance\n",
    "        matricies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        study_pairs: tuple\n",
    "            a tuple or list containing two numpy arrays:\n",
    "            (behavioural_data, imaging_data).\n",
    "            Assumes data is (subjects x features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        covariance_results: dict\n",
    "            dictionary of covariance and auto-covariance matrices\n",
    "\n",
    "        \"\"\"\n",
    "        self._data_able_to_process(X_data, Y_data)\n",
    "\n",
    "        try:\n",
    "            self.covariances_[\"s_Y_Y\"] = (\n",
    "                self._create_covariance_matrix(Y_data, Y_data)\n",
    "            )\n",
    "            self.covariances_[f\"s_X_X\"] = (\n",
    "                self._create_covariance_matrix(X_data, X_data)\n",
    "            )\n",
    "            self.covariances_[f\"s_X_Y\"] = (\n",
    "                self._create_covariance_matrix(X_data, Y_data)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariances due to: {e}\")\n",
    "\n",
    "    def _data_able_to_process(self, X_data, Y_data) -> bool:\n",
    "        \"\"\"\n",
    "        Method to check that data\n",
    "        is in correct format to be processed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         study_pair: tuple,\n",
    "             tuple of behavioural data\n",
    "             and imging data\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool: boolean\n",
    "            bool of if failed or not\n",
    "        \"\"\"\n",
    "        assert isinstance(Y_data, np.ndarray) or not isinstance(\n",
    "            X_data, np.ndarray\n",
    "        ), \"Data provided ins't numpy array\"\n",
    "        assert (X_data.shape[0] != 0) and (\n",
    "            Y_data.shape[0] != 0\n",
    "        ), \"Study pairs contains not data\"\n",
    "        assert (\n",
    "            X_data.shape[0] == Y_data.shape[0]\n",
    "        ), f\"Mismatch between ({X_data.shape[0]} and {Y_data.shape[0]})\"\n",
    "\n",
    "    def _optimise(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to minimise the\n",
    "        objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        model = minimize(\n",
    "            self._objective_function,\n",
    "            self.intial_weights_,\n",
    "            options={\"gtol\": self.tol_, \"maxiter\": self.maxiter_},\n",
    "            args=(self.covariances_, self.theta_, self.l2_),\n",
    "        )\n",
    "        self.best_loss = model.fun\n",
    "        self.weights_ = self._split_weights(model.x)\n",
    "\n",
    "    def _get_dimensions(self, X, Y) -> None:\n",
    "        \"\"\"\n",
    "        Method to get the dimensions\n",
    "        of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *data_sets: tuple\n",
    "            tuple of datasets\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dims_ = [X.shape[1], Y.shape[1]]\n",
    "\n",
    "    def _split_weights(self, weights: np.ndarray) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Splits the flat weight vector weights into individual vectors\n",
    "        for each x and b dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            flatten numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        split_weights: list[np.ndarray]\n",
    "            list of weights split\n",
    "            wx and wb\n",
    "\n",
    "        \"\"\"\n",
    "        wx = weights[0 : self.dims_[0]]\n",
    "        wb = weights[self.dims_[0]:  self.dims_[1]+1]\n",
    "        return [wx, wb]\n",
    "\n",
    "    def _objective_function(\n",
    "        self, weights: np.ndarray, covariances: dict, theta: float, l2: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Objective function of the CA3 class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights: np.ndarray\n",
    "            weights\n",
    "        covariances: dict\n",
    "            dict of cross/auto covariance\n",
    "            matricies\n",
    "        theta: float\n",
    "            theta penality\n",
    "        l2: float\n",
    "            regularization penailty\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        total_loss: float\n",
    "           total loss of the objective function\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        wx, wb = self._split_weights(weights)\n",
    "        s_xb = covariances[\"s_X_Y\"]\n",
    "        s_xx = covariances[\"s_X_X\"]\n",
    "        s_bb = covariances[\"s_Y_Y\"]\n",
    "        total_loss += self._cross_cov_term(wb, s_xb, wx)\n",
    "        total_loss += self._regularization_term(wx, s_xx, l2)\n",
    "        total_loss += self._regularization_term(wb, s_bb, l2)\n",
    "        \n",
    "        ## Similarity penalty across imaging weights, this needs changing\n",
    "        #if theta > 0 and len(weights_) > 1:\n",
    "        #    total_loss += sum(\n",
    "        #        self._dissimilarity_penality(theta, w1[0], w2[0])\n",
    "        #        for w1, w2 in combinations(weights_, 2)\n",
    "        #    )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def _create_covariance_matrix(\n",
    "        self, matrix_1: np.ndarray, matrix_2: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to calculate cross-auto\n",
    "        covariance matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix_1: np.ndarray\n",
    "            A matrix tht should\n",
    "            correspond to subject by\n",
    "            features\n",
    "        matrix_2: np.ndarray\n",
    "            A matrix that should\n",
    "            correspond to features by\n",
    "            feautres\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            array of covariance matrix\n",
    "        \"\"\"\n",
    "        return (matrix_1.T @ matrix_2) / matrix_1.shape[0]\n",
    "\n",
    "    def _normalise(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to normalise data.\n",
    "\n",
    "        Parmeteres\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            data to demean\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: array\n",
    "            demeaned data\n",
    "        \"\"\"\n",
    "        dmean = data - data.mean(axis=0)\n",
    "        std = data.std(axis=0, ddof=1)\n",
    "        std = np.where(std == 0.0, 1.0, std)\n",
    "        return dmean / std\n",
    "\n",
    "    def _cross_cov_term(\n",
    "        self, weight_Y: np.ndarray, cov_mat: np.ndarray, weight_X: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method to calculate the cross covarance term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_Y: np.ndarray\n",
    "            set of weights for wb\n",
    "        cov_mat: np.ndarray\n",
    "             covariance matrix for\n",
    "             wx wb\n",
    "        weight_X: np.ndarray\n",
    "            set of weights for wx\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray: np.array\n",
    "            cross covariance term\n",
    "        \"\"\"\n",
    "        return -weight_X.T @ (cov_mat @ weight_Y)\n",
    "\n",
    "    def _regularization_term(\n",
    "        self, weight: np.ndarray, cov_mat: np.ndarray, lambda_i: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the regularization term\n",
    "        in the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight: np.ndarray\n",
    "            set of weights\n",
    "        cov_mat: np.ndarray\n",
    "            auto covariance matrix\n",
    "        lambda_i: float\n",
    "            regularization parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            regularization term of the objective function\n",
    "        \"\"\"\n",
    "        return 0.5 * lambda_i * (weight.T @ (cov_mat @ weight) - 1)\n",
    "\n",
    "    def _dissimilarity_penality(\n",
    "        self, theta_r: float, X_weight1: np.ndarray, X_weight2: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to return dissimilarity penality\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        theta_r: float\n",
    "           theta penality.\n",
    "        img_weight1: np.ndarray\n",
    "            weights of imaging data\n",
    "        img_weight2: np.ndarray\n",
    "            weights of second imaging\n",
    "            data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            dissimilarity penality\n",
    "        \"\"\"\n",
    "        return theta_r * 0.5 * np.sum((X_weight1 - X_weight2) ** 2)\n",
    "\n",
    "    def _score(self, X, Y) -> float:\n",
    "        \"\"\"\n",
    "        Method used to evaluate model performance.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        data_sets: tuple\n",
    "            a tuple of X, Y data\n",
    "            from an arbituray number of\n",
    "            datasets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: float\n",
    "            mean of correlation\n",
    "            values across datasets\n",
    "\n",
    "        \"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before scoring.\")\n",
    "\n",
    "        self.transform(X, Y)\n",
    "        correlations = self.calculate_canonical_correlations()\n",
    "        return np.mean(correlations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
